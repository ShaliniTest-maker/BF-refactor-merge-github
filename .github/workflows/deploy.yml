# Blue-Green Deployment Workflow for Flask Application Migration
# Implements comprehensive zero-downtime deployment with feature flag control,
# performance validation, and automated rollback procedures per enterprise requirements.
#
# Based on Technical Specification:
# - Section 4.4.1: Blue-Green Deployment Process with container orchestration
# - Section 4.4.2: Performance Monitoring and Validation with ≤10% variance
# - Section 4.4.5: Error Handling and Rollback Procedures
# - Section 8.5.2: Deployment Pipeline with blue-green strategy
# - Section 8.5.3: Release Management Process with stakeholder approval

name: 🚀 Blue-Green Production Deployment

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Target deployment environment'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production
      force_deployment:
        description: 'Force deployment (skip some validations)'
        required: false
        default: false
        type: boolean
      traffic_percentage:
        description: 'Initial traffic percentage for feature flag'
        required: false
        default: '5'
        type: choice
        options:
          - '5'
          - '25'
          - '50'
          - '100'
      skip_performance_validation:
        description: 'Skip performance validation (emergency deployments only)'
        required: false
        default: false
        type: boolean

  workflow_call:
    inputs:
      environment:
        required: true
        type: string
      force_deployment:
        required: false
        type: boolean
        default: false
      traffic_percentage:
        required: false
        type: string
        default: '5'
      skip_performance_validation:
        required: false
        type: boolean
        default: false
    secrets:
      KUBECONFIG:
        required: true
      DOCKER_REGISTRY_TOKEN:
        required: true
      FEATURE_FLAG_API_KEY:
        required: true
      SLACK_WEBHOOK_URL:
        required: false
      GRAFANA_API_KEY:
        required: true
      PROMETHEUS_URL:
        required: true

env:
  # Container and Registry Configuration
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}
  DOCKER_BUILDKIT: 1
  
  # Kubernetes Configuration
  NAMESPACE: flask-app-${{ inputs.environment || 'staging' }}
  BLUE_DEPLOYMENT: flask-app-blue
  GREEN_DEPLOYMENT: flask-app-green
  SERVICE_NAME: flask-app-service
  
  # Performance Monitoring Configuration
  PERFORMANCE_VARIANCE_THRESHOLD: 10
  MONITORING_DURATION: 300  # 5 minutes
  HEALTH_CHECK_TIMEOUT: 60
  ROLLBACK_TIMEOUT: 180
  
  # Feature Flag Configuration
  FEATURE_FLAG_KEY: flask_app_traffic_routing
  TRAFFIC_PROGRESSION: "5,25,50,100"
  
  # Notification Configuration
  SLACK_CHANNEL: "#deployments"
  TEAMS_WEBHOOK: ${{ secrets.TEAMS_WEBHOOK_URL }}

permissions:
  contents: read
  packages: write
  security-events: write
  checks: write
  pull-requests: write

jobs:
  # =============================================================================
  # PRE-DEPLOYMENT VALIDATION
  # =============================================================================
  
  validate-deployment:
    name: 🔍 Pre-Deployment Validation
    runs-on: ubuntu-latest
    environment: ${{ inputs.environment || 'staging' }}
    outputs:
      image-tag: ${{ steps.metadata.outputs.tags }}
      image-digest: ${{ steps.build.outputs.digest }}
      deployment-id: ${{ steps.deployment.outputs.deployment-id }}
      current-environment: ${{ steps.detect.outputs.current-environment }}
    
    steps:
      - name: 📥 Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: 🐍 Setup Python Environment
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: 📦 Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install kubernetes==27.2.0 prometheus-client==0.17.1
      
      - name: 🔐 Login to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.DOCKER_REGISTRY_TOKEN }}
      
      - name: 📋 Extract Metadata
        id: metadata
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}
            type=raw,value=${{ inputs.environment || 'staging' }}-{{sha}}
      
      - name: 🔍 Detect Current Environment State
        id: detect
        run: |
          # Set up kubectl configuration
          echo "${{ secrets.KUBECONFIG }}" | base64 -d > kubeconfig
          export KUBECONFIG=./kubeconfig
          
          # Detect current active environment (blue or green)
          CURRENT_SELECTOR=$(kubectl get service ${{ env.SERVICE_NAME }} \
            -n ${{ env.NAMESPACE }} \
            -o jsonpath='{.spec.selector.environment}' 2>/dev/null || echo "blue")
          
          if [ "$CURRENT_SELECTOR" = "blue" ]; then
            TARGET_ENV="green"
          else
            TARGET_ENV="blue"
          fi
          
          echo "current-environment=$CURRENT_SELECTOR" >> $GITHUB_OUTPUT
          echo "target-environment=$TARGET_ENV" >> $GITHUB_OUTPUT
          
          echo "::notice::Current active environment: $CURRENT_SELECTOR"
          echo "::notice::Target deployment environment: $TARGET_ENV"
      
      - name: 🚀 Generate Deployment ID
        id: deployment
        run: |
          DEPLOYMENT_ID="${{ inputs.environment || 'staging' }}-$(date +%Y%m%d-%H%M%S)-${GITHUB_SHA:0:8}"
          echo "deployment-id=$DEPLOYMENT_ID" >> $GITHUB_OUTPUT
          echo "::notice::Deployment ID: $DEPLOYMENT_ID"
      
      - name: 🐳 Build and Push Container Image
        id: build
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          push: true
          tags: ${{ steps.metadata.outputs.tags }}
          labels: ${{ steps.metadata.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          build-args: |
            DEPLOYMENT_ID=${{ steps.deployment.outputs.deployment-id }}
            BUILD_DATE=$(date -u +'%Y-%m-%dT%H:%M:%SZ')
            VCS_REF=${{ github.sha }}
      
      - name: 🔒 Container Security Scan
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ steps.metadata.outputs.tags }}
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH,MEDIUM'
          exit-code: '1'  # Fail on critical vulnerabilities
      
      - name: 📤 Upload Security Scan Results
        uses: github/codeql-action/upload-sarif@v2
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'

  # =============================================================================
  # STAGING DEPLOYMENT AND VALIDATION
  # =============================================================================
  
  deploy-staging:
    name: 🧪 Deploy to Staging Environment
    runs-on: ubuntu-latest
    needs: validate-deployment
    if: inputs.environment == 'staging' || inputs.environment == null
    environment: staging
    
    steps:
      - name: 📥 Checkout Repository
        uses: actions/checkout@v4
      
      - name: ⚙️ Setup Kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'
      
      - name: 🔐 Configure Kubernetes Access
        run: |
          echo "${{ secrets.KUBECONFIG }}" | base64 -d > kubeconfig
          export KUBECONFIG=./kubeconfig
          kubectl cluster-info
      
      - name: 🚀 Deploy to Staging
        run: |
          export KUBECONFIG=./kubeconfig
          
          # Create namespace if it doesn't exist
          kubectl create namespace ${{ env.NAMESPACE }} --dry-run=client -o yaml | kubectl apply -f -
          
          # Apply staging deployment configuration
          cat <<EOF | kubectl apply -f -
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: ${{ env.GREEN_DEPLOYMENT }}
            namespace: ${{ env.NAMESPACE }}
            labels:
              app: flask-app
              environment: green
              deployment-id: ${{ needs.validate-deployment.outputs.deployment-id }}
          spec:
            replicas: 2
            selector:
              matchLabels:
                app: flask-app
                environment: green
            template:
              metadata:
                labels:
                  app: flask-app
                  environment: green
                  deployment-id: ${{ needs.validate-deployment.outputs.deployment-id }}
              spec:
                containers:
                - name: flask-app
                  image: ${{ needs.validate-deployment.outputs.image-tag }}
                  ports:
                  - containerPort: 8000
                  - containerPort: 8001
                  env:
                  - name: FLASK_ENV
                    value: "staging"
                  - name: DEPLOYMENT_ID
                    value: ${{ needs.validate-deployment.outputs.deployment-id }}
                  resources:
                    requests:
                      memory: "256Mi"
                      cpu: "250m"
                    limits:
                      memory: "512Mi"
                      cpu: "500m"
                  livenessProbe:
                    httpGet:
                      path: /health
                      port: 8000
                    initialDelaySeconds: 30
                    periodSeconds: 10
                  readinessProbe:
                    httpGet:
                      path: /health
                      port: 8000
                    initialDelaySeconds: 5
                    periodSeconds: 5
          ---
          apiVersion: v1
          kind: Service
          metadata:
            name: ${{ env.SERVICE_NAME }}-green
            namespace: ${{ env.NAMESPACE }}
          spec:
            selector:
              app: flask-app
              environment: green
            ports:
            - name: http
              port: 80
              targetPort: 8000
            - name: metrics
              port: 8001
              targetPort: 8001
          EOF
      
      - name: ⏳ Wait for Staging Deployment
        run: |
          export KUBECONFIG=./kubeconfig
          kubectl rollout status deployment/${{ env.GREEN_DEPLOYMENT }} \
            -n ${{ env.NAMESPACE }} \
            --timeout=600s
      
      - name: 🏥 Staging Health Check
        run: |
          export KUBECONFIG=./kubeconfig
          
          # Get service endpoint
          SERVICE_IP=$(kubectl get service ${{ env.SERVICE_NAME }}-green \
            -n ${{ env.NAMESPACE }} \
            -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || \
            kubectl get service ${{ env.SERVICE_NAME }}-green \
            -n ${{ env.NAMESPACE }} \
            -o jsonpath='{.spec.clusterIP}')
          
          # Port forward for testing
          kubectl port-forward service/${{ env.SERVICE_NAME }}-green 8080:80 \
            -n ${{ env.NAMESPACE }} &
          PF_PID=$!
          
          sleep 10
          
          # Comprehensive health check
          for i in {1..10}; do
            if curl -f http://localhost:8080/health; then
              echo "✅ Staging health check passed (attempt $i)"
              kill $PF_PID
              exit 0
            fi
            echo "⏳ Health check attempt $i failed, retrying..."
            sleep 10
          done
          
          kill $PF_PID
          echo "❌ Staging health check failed after 10 attempts"
          exit 1

  # =============================================================================
  # PERFORMANCE VALIDATION
  # =============================================================================
  
  performance-validation:
    name: 📊 Performance Validation
    runs-on: ubuntu-latest
    needs: [validate-deployment, deploy-staging]
    if: inputs.skip_performance_validation != true
    environment: ${{ inputs.environment || 'staging' }}
    
    steps:
      - name: 📥 Checkout Repository
        uses: actions/checkout@v4
      
      - name: 🐍 Setup Python Environment
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: 📦 Install Performance Testing Tools
        run: |
          python -m pip install --upgrade pip
          pip install locust==2.17.0 requests==2.31.0 prometheus-client==0.17.1
          
          # Install k6 for complementary performance testing
          curl -s https://api.github.com/repos/grafana/k6/releases/latest | \
            grep browser_download_url | \
            grep linux-amd64 | \
            cut -d '"' -f 4 | \
            wget -qi -
          tar xvf k6-*-linux-amd64.tar.gz
          sudo mv k6-*-linux-amd64/k6 /usr/local/bin/
      
      - name: ⚙️ Configure Performance Testing
        run: |
          # Set up kubectl for service access
          echo "${{ secrets.KUBECONFIG }}" | base64 -d > kubeconfig
          export KUBECONFIG=./kubeconfig
          
          # Set up port forwarding for performance testing
          kubectl port-forward service/${{ env.SERVICE_NAME }}-green 8080:80 \
            -n ${{ env.NAMESPACE }} &
          PF_PID=$!
          echo "PF_PID=$PF_PID" >> $GITHUB_ENV
          
          sleep 10
      
      - name: 🔥 Execute Locust Load Testing
        run: |
          # Create Locust test configuration
          cat <<EOF > locust_performance_test.py
          from locust import HttpUser, task, between
          import os
          
          class FlaskAppUser(HttpUser):
              wait_time = between(1, 3)
              
              def on_start(self):
                  """Initialize user session"""
                  response = self.client.get("/health")
                  if response.status_code != 200:
                      raise Exception("Health check failed during setup")
              
              @task(3)
              def test_health_endpoint(self):
                  """Test health endpoint performance"""
                  self.client.get("/health")
              
              @task(2)
              def test_api_endpoints(self):
                  """Test primary API endpoints"""
                  self.client.get("/api/health")
              
              @task(1)
              def test_monitoring_endpoint(self):
                  """Test monitoring endpoints"""
                  self.client.get("/metrics", headers={"Accept": "text/plain"})
          EOF
          
          # Execute distributed load test
          locust -f locust_performance_test.py \
                 --headless \
                 --users 50 \
                 --spawn-rate 5 \
                 --run-time 300s \
                 --host http://localhost:8080 \
                 --csv performance_results \
                 --html performance_report.html \
                 --logfile locust.log
      
      - name: ⚡ Execute k6 Performance Analysis
        run: |
          # Create k6 performance test
          cat <<EOF > k6_performance_test.js
          import http from 'k6/http';
          import { check, sleep } from 'k6';
          import { Trend, Rate, Counter } from 'k6/metrics';
          
          // Custom metrics
          let responseTrend = new Trend('response_time_trend');
          let successRate = new Rate('success_rate');
          let errorCounter = new Counter('errors');
          
          export let options = {
            stages: [
              { duration: '2m', target: 10 },   // Ramp-up
              { duration: '5m', target: 50 },   // Load test
              { duration: '2m', target: 0 },    // Ramp-down
            ],
            thresholds: {
              http_req_duration: ['p(95)<500'],     // 95% requests under 500ms
              http_req_failed: ['rate<0.05'],       // Error rate under 5%
              response_time_trend: ['p(95)<600'],   // Custom trend threshold
              success_rate: ['rate>0.95'],          // Success rate above 95%
            },
          };
          
          export default function() {
            let response = http.get('http://localhost:8080/health');
            
            // Record custom metrics
            responseTrend.add(response.timings.duration);
            successRate.add(response.status === 200);
            
            let checkResult = check(response, {
              'status is 200': (r) => r.status === 200,
              'response time < 500ms': (r) => r.timings.duration < 500,
              'response size > 0': (r) => r.body.length > 0,
            });
            
            if (!checkResult) {
              errorCounter.add(1);
            }
            
            sleep(1);
          }
          EOF
          
          # Execute k6 performance test with JSON output
          k6 run --out json=k6_results.json k6_performance_test.js
      
      - name: 📈 Validate Performance Against Baseline
        run: |
          # Create performance validation script
          cat <<EOF > validate_performance.py
          import json
          import sys
          import statistics
          import requests
          from typing import Dict, List, Tuple
          
          def load_locust_results() -> Dict:
              """Load Locust performance results"""
              try:
                  with open('performance_results_stats.csv', 'r') as f:
                      lines = f.readlines()
                      
                  # Parse CSV data (simplified)
                  stats = {}
                  for line in lines[1:]:  # Skip header
                      if line.startswith('GET') and '/health' in line:
                          parts = line.split(',')
                          stats['avg_response_time'] = float(parts[5])
                          stats['max_response_time'] = float(parts[6])
                          stats['requests_per_second'] = float(parts[10])
                          stats['failure_rate'] = float(parts[9])
                          break
                  
                  return stats
              except Exception as e:
                  print(f"Error loading Locust results: {e}")
                  return {}
          
          def load_k6_results() -> Dict:
              """Load k6 performance results"""
              try:
                  with open('k6_results.json', 'r') as f:
                      lines = f.readlines()
                  
                  response_times = []
                  error_rate = 0
                  total_requests = 0
                  failed_requests = 0
                  
                  for line in lines:
                      try:
                          data = json.loads(line)
                          if data.get('type') == 'Point' and data.get('metric') == 'http_req_duration':
                              response_times.append(data['data']['value'])
                          elif data.get('type') == 'Point' and data.get('metric') == 'http_reqs':
                              total_requests += data['data']['value']
                          elif data.get('type') == 'Point' and data.get('metric') == 'http_req_failed':
                              if data['data']['value'] > 0:
                                  failed_requests += 1
                      except:
                          continue
                  
                  if response_times:
                      return {
                          'avg_response_time': statistics.mean(response_times),
                          'p95_response_time': statistics.quantiles(response_times, n=20)[18] if len(response_times) > 20 else max(response_times),
                          'error_rate': (failed_requests / total_requests * 100) if total_requests > 0 else 0
                      }
                  
                  return {}
              except Exception as e:
                  print(f"Error loading k6 results: {e}")
                  return {}
          
          def get_prometheus_baseline() -> Dict:
              """Fetch baseline metrics from Prometheus"""
              try:
                  prometheus_url = "${{ secrets.PROMETHEUS_URL }}"
                  
                  # Query for baseline response time (last 24h average)
                  query = 'avg_over_time(http_request_duration_seconds{job="flask-app-blue"}[24h])'
                  response = requests.get(f"{prometheus_url}/api/v1/query", 
                                        params={'query': query},
                                        timeout=10)
                  
                  if response.status_code == 200:
                      data = response.json()
                      if data['data']['result']:
                          baseline_response_time = float(data['data']['result'][0]['value'][1]) * 1000  # Convert to ms
                          return {'avg_response_time': baseline_response_time}
                  
                  # Fallback baseline if Prometheus unavailable
                  return {'avg_response_time': 100.0}  # 100ms baseline
                  
              except Exception as e:
                  print(f"Warning: Could not fetch Prometheus baseline: {e}")
                  return {'avg_response_time': 100.0}  # Fallback baseline
          
          def validate_performance() -> Tuple[bool, str]:
              """Validate performance against baseline with ≤10% variance"""
              
              # Load test results
              locust_results = load_locust_results()
              k6_results = load_k6_results()
              baseline = get_prometheus_baseline()
              
              print("🔍 Performance Validation Results:")
              print("=" * 50)
              
              # Use most comprehensive results available
              current_results = k6_results if k6_results else locust_results
              
              if not current_results:
                  return False, "❌ No performance results available for validation"
              
              baseline_response_time = baseline.get('avg_response_time', 100.0)
              current_response_time = current_results.get('avg_response_time', 0)
              
              if current_response_time == 0:
                  return False, "❌ Invalid response time data"
              
              # Calculate variance percentage
              variance = ((current_response_time - baseline_response_time) / baseline_response_time) * 100
              
              print(f"📊 Baseline Response Time: {baseline_response_time:.2f}ms")
              print(f"📊 Current Response Time: {current_response_time:.2f}ms")
              print(f"📊 Performance Variance: {variance:+.2f}%")
              print(f"📊 Threshold: ≤{${{ env.PERFORMANCE_VARIANCE_THRESHOLD }}}%")
              
              # Validate against threshold
              if abs(variance) <= ${{ env.PERFORMANCE_VARIANCE_THRESHOLD }}:
                  return True, f"✅ Performance validation PASSED (variance: {variance:+.2f}%)"
              else:
                  return False, f"❌ Performance validation FAILED (variance: {variance:+.2f}% exceeds ±{${{ env.PERFORMANCE_VARIANCE_THRESHOLD }}}%)"
          
          if __name__ == "__main__":
              success, message = validate_performance()
              print(f"\n{message}")
              
              if not success:
                  sys.exit(1)
              
              print("\n✅ Performance validation completed successfully")
          EOF
          
          # Execute performance validation
          python validate_performance.py
      
      - name: 🧹 Cleanup Performance Testing
        if: always()
        run: |
          # Kill port forwarding process
          if [ ! -z "$PF_PID" ]; then
            kill $PF_PID 2>/dev/null || true
          fi
      
      - name: 📤 Upload Performance Reports
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: performance-reports-${{ needs.validate-deployment.outputs.deployment-id }}
          path: |
            performance_results*.csv
            performance_report.html
            k6_results.json
            locust.log
          retention-days: 30

  # =============================================================================
  # PRODUCTION DEPLOYMENT APPROVAL
  # =============================================================================
  
  production-approval:
    name: 🔐 Production Deployment Approval
    runs-on: ubuntu-latest
    needs: [validate-deployment, performance-validation]
    if: inputs.environment == 'production'
    environment: 
      name: production-approval
      url: ${{ steps.deployment.outputs.environment-url }}
    
    steps:
      - name: 📋 Deployment Summary
        run: |
          echo "## 🚀 Production Deployment Request" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Deployment ID:** \`${{ needs.validate-deployment.outputs.deployment-id }}\`" >> $GITHUB_STEP_SUMMARY
          echo "**Container Image:** \`${{ needs.validate-deployment.outputs.image-tag }}\`" >> $GITHUB_STEP_SUMMARY
          echo "**Environment:** \`${{ inputs.environment }}\`" >> $GITHUB_STEP_SUMMARY
          echo "**Initial Traffic:** \`${{ inputs.traffic_percentage }}%\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ✅ Pre-Deployment Validations" >> $GITHUB_STEP_SUMMARY
          echo "- 🔒 Security scan completed" >> $GITHUB_STEP_SUMMARY
          echo "- 📊 Performance validation passed" >> $GITHUB_STEP_SUMMARY
          echo "- 🧪 Staging deployment successful" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**⚠️ This deployment requires manual approval for production release.**" >> $GITHUB_STEP_SUMMARY
      
      - name: 📤 Notify Stakeholders
        run: |
          # Slack notification (if webhook available)
          if [ ! -z "${{ secrets.SLACK_WEBHOOK_URL }}" ]; then
            curl -X POST -H 'Content-type: application/json' \
              --data '{
                "channel": "${{ env.SLACK_CHANNEL }}",
                "username": "GitHub Actions",
                "icon_emoji": ":rocket:",
                "text": "🔐 *Production Deployment Approval Required*",
                "attachments": [
                  {
                    "color": "warning",
                    "fields": [
                      {
                        "title": "Deployment ID",
                        "value": "${{ needs.validate-deployment.outputs.deployment-id }}",
                        "short": true
                      },
                      {
                        "title": "Environment",
                        "value": "${{ inputs.environment }}",
                        "short": true
                      },
                      {
                        "title": "Image",
                        "value": "${{ needs.validate-deployment.outputs.image-tag }}",
                        "short": false
                      }
                    ],
                    "actions": [
                      {
                        "type": "button",
                        "text": "View Deployment",
                        "url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
                      }
                    ]
                  }
                ]
              }' \
              "${{ secrets.SLACK_WEBHOOK_URL }}"
          fi
          
          echo "::notice::Production deployment approval requested"

  # =============================================================================
  # BLUE-GREEN PRODUCTION DEPLOYMENT
  # =============================================================================
  
  deploy-production:
    name: 🚀 Production Blue-Green Deployment
    runs-on: ubuntu-latest
    needs: [validate-deployment, performance-validation, production-approval]
    if: inputs.environment == 'production'
    environment: production
    
    steps:
      - name: 📥 Checkout Repository
        uses: actions/checkout@v4
      
      - name: ⚙️ Setup Kubectl and Tools
        run: |
          # Setup kubectl
          curl -LO "https://dl.k8s.io/release/v1.28.0/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/
          
          # Setup Helm
          curl https://get.helm.sh/helm-v3.12.0-linux-amd64.tar.gz | tar -xz
          sudo mv linux-amd64/helm /usr/local/bin/
          
          # Install additional tools
          pip install kubernetes==27.2.0 prometheus-client==0.17.1
      
      - name: 🔐 Configure Kubernetes Access
        run: |
          echo "${{ secrets.KUBECONFIG }}" | base64 -d > kubeconfig
          export KUBECONFIG=./kubeconfig
          kubectl cluster-info
          
          # Verify namespace exists
          kubectl create namespace ${{ env.NAMESPACE }} --dry-run=client -o yaml | kubectl apply -f -
      
      - name: 🔍 Detect Current Environment
        id: detect
        run: |
          export KUBECONFIG=./kubeconfig
          
          # Detect current active environment
          CURRENT_ENV=$(kubectl get service ${{ env.SERVICE_NAME }} \
            -n ${{ env.NAMESPACE }} \
            -o jsonpath='{.spec.selector.environment}' 2>/dev/null || echo "blue")
          
          if [ "$CURRENT_ENV" = "blue" ]; then
            TARGET_ENV="green"
            TARGET_DEPLOYMENT="${{ env.GREEN_DEPLOYMENT }}"
            CURRENT_DEPLOYMENT="${{ env.BLUE_DEPLOYMENT }}"
          else
            TARGET_ENV="blue"
            TARGET_DEPLOYMENT="${{ env.BLUE_DEPLOYMENT }}"
            CURRENT_DEPLOYMENT="${{ env.GREEN_DEPLOYMENT }}"
          fi
          
          echo "current-env=$CURRENT_ENV" >> $GITHUB_OUTPUT
          echo "target-env=$TARGET_ENV" >> $GITHUB_OUTPUT
          echo "target-deployment=$TARGET_DEPLOYMENT" >> $GITHUB_OUTPUT
          echo "current-deployment=$CURRENT_DEPLOYMENT" >> $GITHUB_OUTPUT
          
          echo "::notice::Current environment: $CURRENT_ENV"
          echo "::notice::Target environment: $TARGET_ENV"
      
      - name: 🚀 Deploy to Target Environment
        run: |
          export KUBECONFIG=./kubeconfig
          
          # Create Helm values for deployment
          cat <<EOF > deployment-values.yaml
          image:
            repository: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
            tag: ${{ github.sha }}
            pullPolicy: Always
          
          deployment:
            name: ${{ steps.detect.outputs.target-deployment }}
            environment: ${{ steps.detect.outputs.target-env }}
            replicas: 3
            
          service:
            name: ${{ env.SERVICE_NAME }}-${{ steps.detect.outputs.target-env }}
            type: LoadBalancer
            
          resources:
            requests:
              memory: "512Mi"
              cpu: "500m"
            limits:
              memory: "1Gi"
              cpu: "1000m"
              
          env:
            FLASK_ENV: production
            DEPLOYMENT_ID: ${{ needs.validate-deployment.outputs.deployment-id }}
            
          monitoring:
            enabled: true
            prometheus:
              scrape: true
              port: 8001
              
          healthcheck:
            enabled: true
            path: /health
            port: 8000
            initialDelaySeconds: 30
            periodSeconds: 10
          EOF
          
          # Apply deployment configuration
          cat <<EOF | kubectl apply -f -
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: ${{ steps.detect.outputs.target-deployment }}
            namespace: ${{ env.NAMESPACE }}
            labels:
              app: flask-app
              environment: ${{ steps.detect.outputs.target-env }}
              deployment-id: ${{ needs.validate-deployment.outputs.deployment-id }}
          spec:
            replicas: 3
            strategy:
              type: RollingUpdate
              rollingUpdate:
                maxUnavailable: 1
                maxSurge: 1
            selector:
              matchLabels:
                app: flask-app
                environment: ${{ steps.detect.outputs.target-env }}
            template:
              metadata:
                labels:
                  app: flask-app
                  environment: ${{ steps.detect.outputs.target-env }}
                  deployment-id: ${{ needs.validate-deployment.outputs.deployment-id }}
                annotations:
                  prometheus.io/scrape: "true"
                  prometheus.io/port: "8001"
                  prometheus.io/path: "/metrics"
              spec:
                containers:
                - name: flask-app
                  image: ${{ needs.validate-deployment.outputs.image-tag }}
                  ports:
                  - containerPort: 8000
                    name: http
                  - containerPort: 8001
                    name: metrics
                  env:
                  - name: FLASK_ENV
                    value: "production"
                  - name: DEPLOYMENT_ID
                    value: ${{ needs.validate-deployment.outputs.deployment-id }}
                  - name: KUBERNETES_NAMESPACE
                    valueFrom:
                      fieldRef:
                        fieldPath: metadata.namespace
                  - name: POD_NAME
                    valueFrom:
                      fieldRef:
                        fieldPath: metadata.name
                  resources:
                    requests:
                      memory: "512Mi"
                      cpu: "500m"
                    limits:
                      memory: "1Gi"
                      cpu: "1000m"
                  livenessProbe:
                    httpGet:
                      path: /health
                      port: 8000
                    initialDelaySeconds: 30
                    periodSeconds: 10
                    timeoutSeconds: 5
                    failureThreshold: 3
                  readinessProbe:
                    httpGet:
                      path: /health
                      port: 8000
                    initialDelaySeconds: 5
                    periodSeconds: 5
                    timeoutSeconds: 3
                    failureThreshold: 3
                  securityContext:
                    runAsNonRoot: true
                    runAsUser: 1001
                    readOnlyRootFilesystem: true
                    allowPrivilegeEscalation: false
          ---
          apiVersion: v1
          kind: Service
          metadata:
            name: ${{ env.SERVICE_NAME }}-${{ steps.detect.outputs.target-env }}
            namespace: ${{ env.NAMESPACE }}
            labels:
              app: flask-app
              environment: ${{ steps.detect.outputs.target-env }}
          spec:
            selector:
              app: flask-app
              environment: ${{ steps.detect.outputs.target-env }}
            ports:
            - name: http
              port: 80
              targetPort: 8000
              protocol: TCP
            - name: metrics
              port: 8001
              targetPort: 8001
              protocol: TCP
            type: LoadBalancer
          EOF
      
      - name: ⏳ Wait for Deployment Rollout
        run: |
          export KUBECONFIG=./kubeconfig
          
          echo "🔄 Waiting for deployment rollout..."
          kubectl rollout status deployment/${{ steps.detect.outputs.target-deployment }} \
            -n ${{ env.NAMESPACE }} \
            --timeout=600s
          
          echo "✅ Deployment rollout completed"
      
      - name: 🏥 Comprehensive Health Check
        run: |
          export KUBECONFIG=./kubeconfig
          
          # Get service endpoint
          echo "🔍 Discovering service endpoint..."
          
          # Wait for load balancer IP
          for i in {1..20}; do
            LB_IP=$(kubectl get service ${{ env.SERVICE_NAME }}-${{ steps.detect.outputs.target-env }} \
              -n ${{ env.NAMESPACE }} \
              -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null)
            
            if [ ! -z "$LB_IP" ] && [ "$LB_IP" != "null" ]; then
              echo "✅ Load balancer IP acquired: $LB_IP"
              echo "LB_IP=$LB_IP" >> $GITHUB_ENV
              break
            fi
            
            echo "⏳ Waiting for load balancer IP (attempt $i/20)..."
            sleep 15
          done
          
          # Fallback to port forwarding if load balancer not available
          if [ -z "$LB_IP" ] || [ "$LB_IP" = "null" ]; then
            echo "⚠️ Load balancer IP not available, using port forwarding"
            kubectl port-forward service/${{ env.SERVICE_NAME }}-${{ steps.detect.outputs.target-env }} 8080:80 \
              -n ${{ env.NAMESPACE }} &
            PF_PID=$!
            echo "PF_PID=$PF_PID" >> $GITHUB_ENV
            sleep 10
            HEALTH_URL="http://localhost:8080/health"
          else
            HEALTH_URL="http://$LB_IP/health"
          fi
          
          # Comprehensive health check
          echo "🏥 Executing comprehensive health check..."
          for i in {1..20}; do
            if curl -f -s "$HEALTH_URL" > /dev/null; then
              echo "✅ Health check passed (attempt $i)"
              
              # Additional endpoint validation
              if curl -f -s "$HEALTH_URL" | grep -q "healthy"; then
                echo "✅ Health endpoint validation successful"
                break
              fi
            fi
            
            echo "⏳ Health check attempt $i failed, retrying in 15 seconds..."
            sleep 15
          done
          
          # Final validation
          if ! curl -f -s "$HEALTH_URL" > /dev/null; then
            echo "❌ Health check failed after 20 attempts"
            exit 1
          fi
          
          echo "✅ Target environment is healthy and ready"

  # =============================================================================
  # FEATURE FLAG TRAFFIC MIGRATION
  # =============================================================================
  
  traffic-migration:
    name: 🚦 Gradual Traffic Migration
    runs-on: ubuntu-latest
    needs: [validate-deployment, deploy-production]
    if: inputs.environment == 'production'
    environment: production
    
    steps:
      - name: 📥 Checkout Repository
        uses: actions/checkout@v4
      
      - name: 🐍 Setup Python Environment
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: 📦 Install Dependencies
        run: |
          pip install requests==2.31.0 prometheus-client==0.17.1 kubernetes==27.2.0
      
      - name: 🔐 Configure Kubernetes Access
        run: |
          echo "${{ secrets.KUBECONFIG }}" | base64 -d > kubeconfig
          export KUBECONFIG=./kubeconfig
      
      - name: 🚦 Initialize Feature Flag Control
        run: |
          # Create traffic migration controller script
          cat <<EOF > traffic_controller.py
          import requests
          import time
          import json
          import os
          import subprocess
          from typing import Dict, List
          
          class TrafficController:
              def __init__(self):
                  self.feature_flag_api_key = "${{ secrets.FEATURE_FLAG_API_KEY }}"
                  self.prometheus_url = "${{ secrets.PROMETHEUS_URL }}"
                  self.target_env = "${{ needs.deploy-production.outputs.target-env }}"
                  self.namespace = "${{ env.NAMESPACE }}"
                  self.service_name = "${{ env.SERVICE_NAME }}"
                  
              def update_traffic_percentage(self, percentage: int) -> bool:
                  """Update traffic routing percentage"""
                  try:
                      # Update Kubernetes service selector based on traffic percentage
                      if percentage == 100:
                          # Full traffic switch
                          cmd = [
                              "kubectl", "patch", "service", self.service_name,
                              "-n", self.namespace,
                              "--type", "json",
                              "-p", f'[{{"op": "replace", "path": "/spec/selector/environment", "value": "{self.target_env}"}}]'
                          ]
                      else:
                          # Implement weighted routing through multiple services
                          # This is a simplified implementation - in production, use Istio or similar
                          print(f"Setting up {percentage}% traffic routing (simplified implementation)")
                          return True
                      
                      result = subprocess.run(cmd, capture_output=True, text=True)
                      if result.returncode == 0:
                          print(f"✅ Traffic routing updated to {percentage}%")
                          return True
                      else:
                          print(f"❌ Failed to update traffic routing: {result.stderr}")
                          return False
                          
                  except Exception as e:
                      print(f"❌ Error updating traffic percentage: {e}")
                      return False
              
              def monitor_metrics(self, duration: int = 60) -> Dict:
                  """Monitor performance metrics during traffic migration"""
                  try:
                      # Query Prometheus for error rate and response time
                      queries = {
                          'error_rate': f'rate(http_requests_total{{job="flask-app-{self.target_env}", status=~"5.."}}, [5m])',
                          'response_time': f'histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{{job="flask-app-{self.target_env}"}}[5m]))',
                          'throughput': f'rate(http_requests_total{{job="flask-app-{self.target_env}"}}[5m])'
                      }
                      
                      metrics = {}
                      for metric_name, query in queries.items():
                          response = requests.get(
                              f"{self.prometheus_url}/api/v1/query",
                              params={'query': query},
                              timeout=10
                          )
                          
                          if response.status_code == 200:
                              data = response.json()
                              if data['data']['result']:
                                  metrics[metric_name] = float(data['data']['result'][0]['value'][1])
                              else:
                                  metrics[metric_name] = 0.0
                          else:
                              metrics[metric_name] = 0.0
                      
                      return metrics
                      
                  except Exception as e:
                      print(f"⚠️ Warning: Could not fetch metrics: {e}")
                      return {'error_rate': 0.0, 'response_time': 0.0, 'throughput': 0.0}
              
              def validate_performance(self, metrics: Dict) -> bool:
                  """Validate performance metrics against thresholds"""
                  # Define performance thresholds
                  thresholds = {
                      'error_rate': 0.05,      # 5% error rate threshold
                      'response_time': 0.5,    # 500ms response time threshold
                      'throughput': 1.0        # Minimum 1 RPS threshold
                  }
                  
                  for metric, value in metrics.items():
                      threshold = thresholds.get(metric)
                      if threshold:
                          if metric == 'error_rate' and value > threshold:
                              print(f"❌ Error rate {value:.2%} exceeds threshold {threshold:.2%}")
                              return False
                          elif metric == 'response_time' and value > threshold:
                              print(f"❌ Response time {value:.3f}s exceeds threshold {threshold:.3f}s")
                              return False
                          elif metric == 'throughput' and value < threshold:
                              print(f"❌ Throughput {value:.2f} RPS below threshold {threshold:.2f} RPS")
                              return False
                  
                  print("✅ Performance validation passed")
                  return True
          
          # Initialize controller
          controller = TrafficController()
          EOF
      
      - name: 🎯 Execute Progressive Traffic Migration
        run: |
          export KUBECONFIG=./kubeconfig
          
          # Traffic migration progression
          TRAFFIC_STAGES="${{ env.TRAFFIC_PROGRESSION }}"
          IFS=',' read -ra STAGES <<< "$TRAFFIC_STAGES"
          
          # Start with initial traffic percentage
          INITIAL_TRAFFIC="${{ inputs.traffic_percentage || '5' }}"
          
          # Add initial stage if not in progression
          if [[ ! " ${STAGES[@]} " =~ " ${INITIAL_TRAFFIC} " ]]; then
            STAGES=("$INITIAL_TRAFFIC" "${STAGES[@]}")
          fi
          
          echo "🚦 Starting progressive traffic migration: ${STAGES[*]}"
          
          for STAGE in "${STAGES[@]}"; do
            echo ""
            echo "🎯 Migrating to $STAGE% traffic..."
            
            # Update traffic routing
            python3 -c "
          import sys
          sys.path.append('.')
          exec(open('traffic_controller.py').read())
          
          controller = TrafficController()
          
          # Update traffic percentage
          if not controller.update_traffic_percentage($STAGE):
              print('❌ Failed to update traffic routing')
              exit(1)
          
          print('⏳ Monitoring performance for ${{ env.MONITORING_DURATION }} seconds...')
          import time
          time.sleep(60)  # Initial monitoring period
          
          # Monitor performance
          metrics = controller.monitor_metrics(${{ env.MONITORING_DURATION }})
          print(f'📊 Performance metrics: {metrics}')
          
          # Validate performance
          if not controller.validate_performance(metrics):
              print('❌ Performance validation failed - initiating rollback')
              exit(1)
          
          print(f'✅ $STAGE% traffic migration successful')
          "
            
            if [ $? -ne 0 ]; then
              echo "❌ Traffic migration failed at $STAGE% - initiating rollback"
              exit 1
            fi
            
            # Wait between stages (except for final stage)
            if [ "$STAGE" != "100" ]; then
              echo "⏳ Waiting 2 minutes before next stage..."
              sleep 120
            fi
          done
          
          echo ""
          echo "🎉 Progressive traffic migration completed successfully!"
      
      - name: 📊 Final Performance Validation
        run: |
          export KUBECONFIG=./kubeconfig
          
          echo "📊 Executing final performance validation..."
          
          python3 -c "
          exec(open('traffic_controller.py').read())
          
          controller = TrafficController()
          
          # Extended monitoring for final validation
          print('⏳ Extended monitoring period (5 minutes)...')
          import time
          time.sleep(300)
          
          # Final metrics collection
          metrics = controller.monitor_metrics(300)
          print(f'📊 Final performance metrics: {metrics}')
          
          # Strict validation for final stage
          if not controller.validate_performance(metrics):
              print('❌ Final performance validation failed')
              exit(1)
          
          print('✅ Final performance validation successful')
          "

  # =============================================================================
  # POST-DEPLOYMENT CLEANUP
  # =============================================================================
  
  post-deployment:
    name: 🧹 Post-Deployment Operations
    runs-on: ubuntu-latest
    needs: [validate-deployment, deploy-production, traffic-migration]
    if: always() && needs.deploy-production.result == 'success'
    environment: production
    
    steps:
      - name: 📥 Checkout Repository
        uses: actions/checkout@v4
      
      - name: 🔐 Configure Kubernetes Access
        run: |
          echo "${{ secrets.KUBECONFIG }}" | base64 -d > kubeconfig
          export KUBECONFIG=./kubeconfig
      
      - name: 🗑️ Cleanup Previous Environment
        run: |
          export KUBECONFIG=./kubeconfig
          
          # Determine old environment to cleanup
          CURRENT_ENV=$(kubectl get service ${{ env.SERVICE_NAME }} \
            -n ${{ env.NAMESPACE }} \
            -o jsonpath='{.spec.selector.environment}' 2>/dev/null || echo "")
          
          if [ "$CURRENT_ENV" = "blue" ]; then
            OLD_ENV="green"
            OLD_DEPLOYMENT="${{ env.GREEN_DEPLOYMENT }}"
          elif [ "$CURRENT_ENV" = "green" ]; then
            OLD_ENV="blue"
            OLD_DEPLOYMENT="${{ env.BLUE_DEPLOYMENT }}"
          else
            echo "⚠️ Could not determine old environment for cleanup"
            exit 0
          fi
          
          echo "🗑️ Cleaning up old environment: $OLD_ENV"
          
          # Scale down old deployment
          kubectl scale deployment $OLD_DEPLOYMENT \
            -n ${{ env.NAMESPACE }} \
            --replicas=0 || true
          
          # Wait for scale down
          sleep 30
          
          # Delete old deployment and service
          kubectl delete deployment $OLD_DEPLOYMENT \
            -n ${{ env.NAMESPACE }} || true
          kubectl delete service ${{ env.SERVICE_NAME }}-$OLD_ENV \
            -n ${{ env.NAMESPACE }} || true
          
          echo "✅ Cleanup completed for $OLD_ENV environment"
      
      - name: 📈 Update Performance Baseline
        run: |
          # Create baseline update script
          cat <<EOF > update_baseline.py
          import requests
          import json
          import time
          from datetime import datetime, timedelta
          
          def update_performance_baseline():
              """Update performance baseline with current metrics"""
              try:
                  prometheus_url = "${{ secrets.PROMETHEUS_URL }}"
                  
                  # Query current performance metrics
                  end_time = datetime.now()
                  start_time = end_time - timedelta(hours=1)
                  
                  queries = {
                      'avg_response_time': 'avg_over_time(http_request_duration_seconds[1h])',
                      'p95_response_time': 'histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[1h]))',
                      'throughput': 'rate(http_requests_total[1h])',
                      'error_rate': 'rate(http_requests_total{status=~"5.."}[1h])'
                  }
                  
                  baseline_metrics = {}
                  for metric_name, query in queries.items():
                      response = requests.get(
                          f"{prometheus_url}/api/v1/query",
                          params={'query': query},
                          timeout=10
                      )
                      
                      if response.status_code == 200:
                          data = response.json()
                          if data['data']['result']:
                              baseline_metrics[metric_name] = float(data['data']['result'][0]['value'][1])
                  
                  # Save baseline to file for future comparisons
                  baseline_data = {
                      'timestamp': datetime.now().isoformat(),
                      'deployment_id': '${{ needs.validate-deployment.outputs.deployment-id }}',
                      'metrics': baseline_metrics
                  }
                  
                  with open('performance_baseline.json', 'w') as f:
                      json.dump(baseline_data, f, indent=2)
                  
                  print("✅ Performance baseline updated successfully")
                  print(f"📊 New baseline metrics: {baseline_metrics}")
                  
              except Exception as e:
                  print(f"⚠️ Warning: Could not update performance baseline: {e}")
          
          if __name__ == "__main__":
              update_performance_baseline()
          EOF
          
          # Install dependencies and update baseline
          pip install requests==2.31.0
          python update_baseline.py
      
      - name: 📤 Upload Deployment Artifacts
        uses: actions/upload-artifact@v3
        with:
          name: deployment-artifacts-${{ needs.validate-deployment.outputs.deployment-id }}
          path: |
            performance_baseline.json
            kubeconfig
          retention-days: 90
      
      - name: 📢 Deployment Success Notification
        run: |
          # Success notification
          if [ ! -z "${{ secrets.SLACK_WEBHOOK_URL }}" ]; then
            curl -X POST -H 'Content-type: application/json' \
              --data '{
                "channel": "${{ env.SLACK_CHANNEL }}",
                "username": "GitHub Actions",
                "icon_emoji": ":tada:",
                "text": "🎉 *Production Deployment Successful*",
                "attachments": [
                  {
                    "color": "good",
                    "fields": [
                      {
                        "title": "Deployment ID",
                        "value": "${{ needs.validate-deployment.outputs.deployment-id }}",
                        "short": true
                      },
                      {
                        "title": "Environment",
                        "value": "${{ inputs.environment }}",
                        "short": true
                      },
                      {
                        "title": "Traffic Migration",
                        "value": "Completed: 5% → 25% → 50% → 100%",
                        "short": false
                      },
                      {
                        "title": "Performance",
                        "value": "✅ Validated (≤10% variance)",
                        "short": true
                      }
                    ],
                    "actions": [
                      {
                        "type": "button",
                        "text": "View Monitoring",
                        "url": "${{ secrets.GRAFANA_URL }}/d/flask-app-dashboard"
                      }
                    ]
                  }
                ]
              }' \
              "${{ secrets.SLACK_WEBHOOK_URL }}"
          fi
          
          echo "::notice::🎉 Blue-green deployment completed successfully!"

  # =============================================================================
  # ROLLBACK PROCEDURES
  # =============================================================================
  
  emergency-rollback:
    name: 🔄 Emergency Rollback
    runs-on: ubuntu-latest
    if: failure() && needs.deploy-production.result == 'success'
    needs: [validate-deployment, deploy-production]
    environment: production
    
    steps:
      - name: 📥 Checkout Repository
        uses: actions/checkout@v4
      
      - name: 🔐 Configure Kubernetes Access
        run: |
          echo "${{ secrets.KUBECONFIG }}" | base64 -d > kubeconfig
          export KUBECONFIG=./kubeconfig
      
      - name: 🔄 Execute Emergency Rollback
        run: |
          export KUBECONFIG=./kubeconfig
          
          echo "🚨 Initiating emergency rollback..."
          
          # Determine current and previous environments
          CURRENT_ENV=$(kubectl get service ${{ env.SERVICE_NAME }} \
            -n ${{ env.NAMESPACE }} \
            -o jsonpath='{.spec.selector.environment}' 2>/dev/null || echo "")
          
          if [ "$CURRENT_ENV" = "blue" ]; then
            ROLLBACK_ENV="green"
            ROLLBACK_DEPLOYMENT="${{ env.GREEN_DEPLOYMENT }}"
          elif [ "$CURRENT_ENV" = "green" ]; then
            ROLLBACK_ENV="blue"
            ROLLBACK_DEPLOYMENT="${{ env.BLUE_DEPLOYMENT }}"
          else
            echo "❌ Cannot determine rollback target"
            exit 1
          fi
          
          echo "🔄 Rolling back to $ROLLBACK_ENV environment..."
          
          # Check if rollback target exists and is healthy
          if kubectl get deployment $ROLLBACK_DEPLOYMENT -n ${{ env.NAMESPACE }} > /dev/null 2>&1; then
            # Scale up rollback environment
            kubectl scale deployment $ROLLBACK_DEPLOYMENT \
              -n ${{ env.NAMESPACE }} \
              --replicas=3
            
            # Wait for rollback environment to be ready
            kubectl rollout status deployment/$ROLLBACK_DEPLOYMENT \
              -n ${{ env.NAMESPACE }} \
              --timeout=300s
            
            # Switch traffic back
            kubectl patch service ${{ env.SERVICE_NAME }} \
              -n ${{ env.NAMESPACE }} \
              --type json \
              -p "[{\"op\": \"replace\", \"path\": \"/spec/selector/environment\", \"value\": \"$ROLLBACK_ENV\"}]"
            
            echo "✅ Emergency rollback completed"
          else
            echo "❌ Rollback target not available"
            exit 1
          fi
      
      - name: 📢 Rollback Notification
        if: always()
        run: |
          # Rollback notification
          if [ ! -z "${{ secrets.SLACK_WEBHOOK_URL }}" ]; then
            curl -X POST -H 'Content-type: application/json' \
              --data '{
                "channel": "${{ env.SLACK_CHANNEL }}",
                "username": "GitHub Actions",
                "icon_emoji": ":warning:",
                "text": "🚨 *Emergency Rollback Executed*",
                "attachments": [
                  {
                    "color": "danger",
                    "fields": [
                      {
                        "title": "Deployment ID",
                        "value": "${{ needs.validate-deployment.outputs.deployment-id }}",
                        "short": true
                      },
                      {
                        "title": "Reason",
                        "value": "Deployment failure detected",
                        "short": true
                      },
                      {
                        "title": "Action",
                        "value": "Traffic reverted to previous environment",
                        "short": false
                      }
                    ],
                    "actions": [
                      {
                        "type": "button",
                        "text": "View Logs",
                        "url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
                      }
                    ]
                  }
                ]
              }' \
              "${{ secrets.SLACK_WEBHOOK_URL }}"
          fi
          
          echo "::error::🚨 Emergency rollback executed due to deployment failure"