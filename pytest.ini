[tool:pytest]
# ===========================================
# pytest Configuration for Flask Application
# Migration from Node.js to Python/Flask
# ===========================================

# Test Discovery Configuration
minversion = 7.4
testpaths = tests
python_files = test_*.py *_test.py
python_classes = Test*
python_functions = test_*

# Test Collection Patterns
collect_ignore = [
    "setup.py",
    "conftest.py",
    "build",
    "dist",
    ".git",
    ".venv",
    "venv",
    "__pycache__",
    ".pytest_cache",
    "node_modules",
    ".coverage"
]

# Flask Application Testing Configuration
# Enables pytest-flask integration for Flask-specific testing patterns
addopts = 
    --verbose
    --strict-markers
    --strict-config
    --tb=short
    --maxfail=10
    --disable-warnings
    --durations=10
    --color=yes
    --junitxml=reports/pytest-junit.xml
    --html=reports/pytest-report.html
    --self-contained-html
    --cov=src
    --cov-report=html:reports/coverage-html
    --cov-report=xml:reports/coverage.xml
    --cov-report=term-missing
    --cov-branch
    --cov-fail-under=90
    --cov-config=.coveragerc

# Parallel Test Execution Configuration
# pytest-xdist for distributed test execution reducing runtime
# Optimized for CI/CD pipeline performance
markers =
    unit: Unit tests for individual components and functions
    integration: Integration tests for component interactions
    e2e: End-to-end tests for complete workflows
    performance: Performance and benchmark tests
    security: Security and vulnerability tests
    slow: Tests that take longer than 5 seconds
    database: Tests requiring database connections
    cache: Tests requiring Redis cache connections
    auth: Authentication and authorization tests
    external: Tests requiring external service mocking
    smoke: Smoke tests for basic functionality
    regression: Regression tests for critical bugs
    flaky: Tests marked as potentially unstable
    asyncio: Asynchronous tests using Motor database operations

# Test Environment Configuration
env = 
    TESTING=true
    FLASK_ENV=testing
    FLASK_DEBUG=false
    LOG_LEVEL=WARNING
    WTF_CSRF_ENABLED=false

# Coverage Configuration (≥90% requirement per Section 8.5.1)
# Implements enterprise-grade coverage requirements
filterwarnings =
    ignore::UserWarning
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore:.*unclosed.*:ResourceWarning
    ignore:.*aioredis.*:DeprecationWarning
    ignore:.*pymongo.*:DeprecationWarning
    error::pytest.PytestUnraisableExceptionWarning

# Async Testing Configuration
# pytest-asyncio for Motor database operations and async external service calls
asyncio_mode = auto
asyncio_default_fixture_loop_scope = function

# Test Data and Fixture Configuration
# Supports factory_boy and Testcontainers integration
usefixtures = app_context

# Test Timeout Configuration
# Prevents hanging tests in CI/CD pipeline
timeout = 300
timeout_method = thread

# Test Performance Configuration
# Supports load testing and benchmark validation
doctest_optionflags = NORMALIZE_WHITESPACE IGNORE_EXCEPTION_DETAIL

# Mock Configuration
# Comprehensive external service mocking capabilities
mock_use_standalone_module = true

# Logging Configuration for Test Environment
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

log_file = reports/pytest.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] %(filename)s:%(lineno)d %(funcName)s(): %(message)s
log_file_date_format = %Y-%m-%d %H:%M:%S

# Test Report Configuration
# Enterprise-grade reporting for CI/CD integration
junit_suite_name = Flask Application Test Suite
junit_logging = system-out
junit_log_passing_tests = false
junit_duration_report = total
junit_family = xunit2

# Performance Testing Configuration
# Baseline comparison and variance validation (≤10% requirement)
performance_baseline_file = tests/performance/baseline_data.json
performance_variance_threshold = 10
performance_timeout = 120

# Security Testing Configuration
# Integration with bandit and safety scanning
security_scan_enabled = true
security_fail_on_critical = true

# Container Testing Configuration
# Testcontainers integration for MongoDB and Redis
testcontainers_mongodb_image = mongo:7.0
testcontainers_redis_image = redis:7.2-alpine
testcontainers_cleanup_enabled = true
testcontainers_reuse_enabled = false

# Quality Gate Configuration
# Enterprise-grade quality enforcement
xfail_strict = true
consider_namespace_packages = true

# Test Execution Optimization
# Resource management and performance tuning
cache_dir = .pytest_cache
tmp_path_retention_count = 3
tmp_path_retention_policy = failed

# External Service Mock Configuration
# Comprehensive mocking for Auth0, AWS, and third-party APIs
mock_auth0_enabled = true
mock_aws_enabled = true
mock_external_apis = true

# Database Test Configuration
# Production-equivalent database behavior through Testcontainers
database_isolation = true
database_cleanup_enabled = true
database_seed_data = true

# CI/CD Pipeline Integration
# GitHub Actions compatibility and reporting
ci_junit_report_path = reports/junit/test-results.xml
ci_coverage_report_path = reports/coverage/coverage.xml
ci_html_report_path = reports/html/pytest-report.html

# Test Data Management
# Fixture and factory pattern configuration
fixture_cleanup_enabled = true
factory_boy_faker_seed = 12345
test_data_retention_policy = cleanup

# Error Handling Configuration
# Comprehensive error reporting and debugging
traceback_style = short
show_capture = no
capture_manager_enabled = true

# Plugin Configuration
# Essential plugins for enterprise testing
required_plugins = 
    pytest-flask>=1.3.0
    pytest-cov>=4.1.0
    pytest-xdist>=3.3.1
    pytest-asyncio>=0.21.1
    pytest-mock>=3.11.1
    pytest-html>=3.2.0
    pytest-timeout>=2.1.0
    pytest-env>=0.8.2

# Test Categories and Execution Groups
# Organized test execution for different CI/CD stages
test_groups = 
    smoke: "unit and not slow"
    unit: "unit and not integration and not e2e"
    integration: "integration and not e2e"
    security: "security"
    performance: "performance"
    regression: "regression"
    all: "not flaky"

# Performance Monitoring Integration
# Prometheus metrics and monitoring hooks
performance_monitoring_enabled = true
metrics_collection_enabled = true
baseline_comparison_enabled = true

# Test Result Archival
# Historical test data and trend analysis
archive_test_results = true
archive_coverage_reports = true
archive_performance_data = true
archive_retention_days = 30

# Development Environment Support
# Local development testing configuration
dev_mode_enabled = false
dev_fast_mode = false
dev_skip_slow_tests = false
dev_parallel_workers = auto

# Quality Assurance Configuration
# Comprehensive QA validation and reporting
qa_coverage_threshold = 90
qa_performance_threshold = 10
qa_security_scan_enabled = true
qa_code_quality_check = true

# Deployment Gate Configuration
# Production deployment readiness validation
deployment_gate_enabled = true
deployment_coverage_required = 90
deployment_performance_validated = true
deployment_security_cleared = true