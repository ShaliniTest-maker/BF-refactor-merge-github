"""
PyMongo 4.5+ synchronous database operations module.

This module implements comprehensive MongoDB database operations using PyMongo 4.5+ to replace
Node.js MongoDB driver functionality while preserving existing query patterns and document structures.
Provides thread-safe CRUD operations, transaction management, connection pooling, and performance
monitoring integration to ensure â‰¤10% variance from Node.js baseline.

Key Features:
- Complete CRUD operations maintaining identical query patterns from Node.js implementation
- Transaction management with commit/rollback support for data consistency
- Connection pool monitoring and health checks for resource optimization
- PyMongo event listeners for Prometheus metrics collection and performance tracking
- Comprehensive error handling with retry logic and circuit breaker patterns
- Schema preservation ensuring zero database modifications during migration

Implements requirements from:
- Section 0.1.2: Data access components conversion from Node.js to PyMongo 4.5+
- Section 6.2.1: Schema design preservation maintaining existing document structures
- Section 6.2.4: Performance optimization with connection pooling and monitoring
- Section 4.2.2: State management with transaction support and error handling
"""

import logging
import time
from contextlib import contextmanager
from typing import Any, Dict, List, Optional, Union, Tuple, Callable, Iterator
from dataclasses import dataclass, field
from threading import Lock, RLock
from datetime import datetime, timezone
import json

import pymongo
from pymongo import MongoClient, ASCENDING, DESCENDING, TEXT
from pymongo.client_session import ClientSession
from pymongo.collection import Collection
from pymongo.database import Database
from pymongo.cursor import Cursor
from pymongo.command_cursor import CommandCursor
from pymongo.errors import (
    ConnectionFailure,
    ServerSelectionTimeoutError,
    DuplicateKeyError,
    BulkWriteError,
    OperationFailure,
    NetworkTimeout,
    ExecutionTimeout,
    WriteError,
    WriteConcernError,
    ConfigurationError
)
from pymongo.read_concern import ReadConcern
from pymongo.read_preferences import ReadPreference
from pymongo.write_concern import WriteConcern
from pymongo.operations import (
    InsertOne,
    UpdateOne,
    UpdateMany,
    DeleteOne,
    DeleteMany,
    ReplaceOne
)
from bson import ObjectId, json_util
from bson.errors import InvalidId
import structlog

# Import monitoring and error handling modules
from .monitoring import (
    get_database_monitoring_components,
    monitor_transaction,
    DatabaseHealthChecker,
    initialize_database_monitoring
)
from .exceptions import (
    DatabaseException,
    DatabaseConnectionError,
    DatabaseQueryError,
    DatabaseTransactionError,
    DatabaseTimeoutError,
    DatabaseValidationError,
    with_database_retry,
    database_error_context,
    get_circuit_breaker
)

# Configure structured logger
logger = structlog.get_logger(__name__)

# MongoDB connection and pool configuration constants
DEFAULT_CONNECTION_TIMEOUT_MS = 30000
DEFAULT_SERVER_SELECTION_TIMEOUT_MS = 30000
DEFAULT_SOCKET_TIMEOUT_MS = 30000
DEFAULT_MAX_POOL_SIZE = 50
DEFAULT_MIN_POOL_SIZE = 5
DEFAULT_MAX_IDLE_TIME_MS = 300000  # 5 minutes
DEFAULT_WAIT_QUEUE_TIMEOUT_MS = 30000

# Transaction configuration constants
DEFAULT_TRANSACTION_TIMEOUT_SECONDS = 60
MAX_TRANSACTION_RETRY_ATTEMPTS = 3

# Query optimization constants
DEFAULT_BATCH_SIZE = 1000
MAX_BATCH_SIZE = 10000


@dataclass
class MongoDBConfig:
    """
    MongoDB configuration container for connection and operational settings.
    
    Centralizes all MongoDB configuration parameters including connection pooling,
    timeout settings, and performance optimization parameters to ensure consistent
    configuration across all database operations.
    """
    
    # Connection settings
    uri: str = field(default="mongodb://localhost:27017")
    database_name: str = field(default="app_database")
    
    # Connection pool settings
    max_pool_size: int = field(default=DEFAULT_MAX_POOL_SIZE)
    min_pool_size: int = field(default=DEFAULT_MIN_POOL_SIZE)
    max_idle_time_ms: int = field(default=DEFAULT_MAX_IDLE_TIME_MS)
    wait_queue_timeout_ms: int = field(default=DEFAULT_WAIT_QUEUE_TIMEOUT_MS)
    
    # Timeout settings
    connection_timeout_ms: int = field(default=DEFAULT_CONNECTION_TIMEOUT_MS)
    server_selection_timeout_ms: int = field(default=DEFAULT_SERVER_SELECTION_TIMEOUT_MS)
    socket_timeout_ms: int = field(default=DEFAULT_SOCKET_TIMEOUT_MS)
    
    # Read/Write preferences
    read_preference: str = field(default="primary")
    write_concern_w: Union[int, str] = field(default=1)
    write_concern_j: bool = field(default=True)
    write_concern_fsync: bool = field(default=False)
    
    # Performance settings
    default_batch_size: int = field(default=DEFAULT_BATCH_SIZE)
    max_batch_size: int = field(default=MAX_BATCH_SIZE)
    
    # Monitoring settings
    enable_monitoring: bool = field(default=True)
    enable_command_monitoring: bool = field(default=True)
    enable_pool_monitoring: bool = field(default=True)
    enable_server_monitoring: bool = field(default=True)
    
    def to_client_kwargs(self) -> Dict[str, Any]:
        """Convert configuration to PyMongo client initialization arguments."""
        read_pref_map = {
            'primary': ReadPreference.PRIMARY,
            'primaryPreferred': ReadPreference.PRIMARY_PREFERRED,
            'secondary': ReadPreference.SECONDARY,
            'secondaryPreferred': ReadPreference.SECONDARY_PREFERRED,
            'nearest': ReadPreference.NEAREST
        }
        
        return {
            'maxPoolSize': self.max_pool_size,
            'minPoolSize': self.min_pool_size,
            'maxIdleTimeMS': self.max_idle_time_ms,
            'waitQueueTimeoutMS': self.wait_queue_timeout_ms,
            'connectTimeoutMS': self.connection_timeout_ms,
            'serverSelectionTimeoutMS': self.server_selection_timeout_ms,
            'socketTimeoutMS': self.socket_timeout_ms,
            'readPreference': read_pref_map.get(self.read_preference, ReadPreference.PRIMARY),
            'w': self.write_concern_w,
            'j': self.write_concern_j,
            'fsync': self.write_concern_fsync,
            'retryWrites': True,
            'retryReads': True,
            'appName': 'Flask-Migration-App'
        }


@dataclass 
class QueryResult:
    """
    Standardized query result container preserving Node.js compatibility.
    
    Provides consistent response structure for all database operations,
    maintaining compatibility with existing Node.js response patterns
    while adding Python-specific enhancements.
    """
    
    success: bool
    data: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None
    count: Optional[int] = None
    inserted_id: Optional[ObjectId] = None
    modified_count: Optional[int] = None
    deleted_count: Optional[int] = None
    matched_count: Optional[int] = None
    upserted_id: Optional[ObjectId] = None
    acknowledged: bool = True
    error: Optional[str] = None
    execution_time_ms: Optional[float] = None
    operation: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert result to dictionary format compatible with Node.js responses."""
        result = {
            'success': self.success,
            'acknowledged': self.acknowledged
        }
        
        if self.data is not None:
            # Convert ObjectIds and other BSON types for JSON serialization
            result['data'] = json.loads(json_util.dumps(self.data))
        
        if self.count is not None:
            result['count'] = self.count
            
        if self.inserted_id is not None:
            result['insertedId'] = str(self.inserted_id)
            
        if self.modified_count is not None:
            result['modifiedCount'] = self.modified_count
            
        if self.deleted_count is not None:
            result['deletedCount'] = self.deleted_count
            
        if self.matched_count is not None:
            result['matchedCount'] = self.matched_count
            
        if self.upserted_id is not None:
            result['upsertedId'] = str(self.upserted_id)
            
        if self.error is not None:
            result['error'] = self.error
            
        if self.execution_time_ms is not None:
            result['executionTimeMs'] = self.execution_time_ms
            
        if self.operation is not None:
            result['operation'] = self.operation
        
        return result


class MongoDBClient:
    """
    Thread-safe PyMongo 4.5+ client wrapper implementing comprehensive database operations.
    
    Provides a high-level interface for MongoDB operations while preserving exact compatibility
    with Node.js query patterns and response formats. Implements connection pooling, transaction
    management, performance monitoring, and comprehensive error handling.
    
    Features:
    - Thread-safe operations with connection pooling
    - Transaction management with automatic retry and rollback
    - Performance monitoring with Prometheus metrics integration
    - Circuit breaker pattern for connection resilience
    - Comprehensive error handling and logging
    - Query pattern preservation from Node.js implementation
    """
    
    def __init__(self, config: MongoDBConfig):
        """
        Initialize MongoDB client with comprehensive configuration and monitoring.
        
        Args:
            config: MongoDB configuration containing connection and operational parameters
        """
        self.config = config
        self._client: Optional[MongoClient] = None
        self._database: Optional[Database] = None
        self._connection_lock = RLock()
        self._initialized = False
        
        # Initialize monitoring components
        self._monitoring_components = None
        self._health_checker: Optional[DatabaseHealthChecker] = None
        
        # Connection state tracking
        self._connection_stats = {
            'total_connections': 0,
            'active_connections': 0,
            'failed_connections': 0,
            'last_connection_time': None,
            'last_failure_time': None
        }
        
        logger.info(
            "MongoDB client initialized",
            database_name=config.database_name,
            max_pool_size=config.max_pool_size,
            min_pool_size=config.min_pool_size
        )
    
    def initialize(self) -> None:
        """
        Initialize MongoDB client connection and monitoring components.
        
        Establishes database connection, registers monitoring listeners,
        and configures health checking for comprehensive database operations.
        
        Raises:
            DatabaseConnectionError: If connection initialization fails
        """
        with self._connection_lock:
            if self._initialized:
                logger.debug("MongoDB client already initialized")
                return
            
            try:
                # Initialize monitoring if enabled
                if self.config.enable_monitoring:
                    self._monitoring_components = initialize_database_monitoring()
                    logger.info("Database monitoring components initialized")
                
                # Create MongoDB client with configuration
                client_kwargs = self.config.to_client_kwargs()
                self._client = MongoClient(self.config.uri, **client_kwargs)
                
                # Get database reference
                self._database = self._client[self.config.database_name]
                
                # Initialize health checker
                if self._monitoring_components:
                    self._health_checker = self._monitoring_components['health_checker']
                
                # Verify connection with ping
                self._verify_connection()
                
                # Update connection stats
                self._connection_stats['total_connections'] += 1
                self._connection_stats['active_connections'] += 1
                self._connection_stats['last_connection_time'] = time.time()
                
                self._initialized = True
                
                logger.info(
                    "MongoDB client connection established",
                    database_name=self.config.database_name,
                    server_info=self._get_server_info()
                )
                
            except Exception as e:
                self._connection_stats['failed_connections'] += 1
                self._connection_stats['last_failure_time'] = time.time()
                
                logger.error(
                    "MongoDB client initialization failed",
                    error=str(e),
                    database_name=self.config.database_name
                )
                
                raise DatabaseConnectionError(
                    message=f"Failed to initialize MongoDB client: {str(e)}",
                    database=self.config.database_name,
                    operation="initialize",
                    original_error=e
                )
    
    def _verify_connection(self) -> None:
        """Verify MongoDB connection with ping command."""
        if not self._client:
            raise DatabaseConnectionError("MongoDB client not initialized")
        
        try:
            # Ping database to verify connection
            self._client.admin.command('ping')
            logger.debug("MongoDB connection verified successfully")
            
        except Exception as e:
            logger.error(f"MongoDB connection verification failed: {e}")
            raise DatabaseConnectionError(
                message=f"Connection verification failed: {str(e)}",
                database=self.config.database_name,
                operation="ping",
                original_error=e
            )
    
    def _get_server_info(self) -> Dict[str, Any]:
        """Get MongoDB server information for logging and monitoring."""
        try:
            if self._client:
                server_info = self._client.server_info()
                return {
                    'version': server_info.get('version', 'unknown'),
                    'platform': server_info.get('platform', 'unknown'),
                    'uptime': server_info.get('uptime', 0)
                }
        except Exception as e:
            logger.warning(f"Could not retrieve server info: {e}")
        
        return {'version': 'unknown', 'platform': 'unknown', 'uptime': 0}
    
    @property
    def client(self) -> MongoClient:
        """Get MongoDB client instance, initializing if necessary."""
        if not self._initialized:
            self.initialize()
        
        if not self._client:
            raise DatabaseConnectionError("MongoDB client not available")
        
        return self._client
    
    @property
    def database(self) -> Database:
        """Get MongoDB database instance, initializing if necessary."""
        if not self._initialized:
            self.initialize()
        
        if not self._database:
            raise DatabaseConnectionError("MongoDB database not available")
        
        return self._database
    
    def get_collection(self, collection_name: str) -> Collection:
        """
        Get MongoDB collection reference with monitoring integration.
        
        Args:
            collection_name: Name of the collection to access
            
        Returns:
            Collection: MongoDB collection reference
        """
        try:
            collection = self.database[collection_name]
            logger.debug(
                "Collection reference obtained",
                collection_name=collection_name,
                database_name=self.config.database_name
            )
            return collection
            
        except Exception as e:
            logger.error(
                "Failed to get collection reference",
                collection_name=collection_name,
                error=str(e)
            )
            raise DatabaseQueryError(
                message=f"Failed to get collection '{collection_name}': {str(e)}",
                collection=collection_name,
                database=self.config.database_name,
                operation="get_collection",
                original_error=e
            )
    
    @with_database_retry(max_attempts=3, min_wait=1.0, max_wait=5.0)
    def find_one(
        self,
        collection_name: str,
        filter_doc: Optional[Dict[str, Any]] = None,
        projection: Optional[Dict[str, Any]] = None,
        sort: Optional[List[Tuple[str, int]]] = None,
        **kwargs
    ) -> QueryResult:
        """
        Find a single document matching the filter criteria.
        
        Implements identical query patterns from Node.js findOne() operations
        while providing comprehensive error handling and performance monitoring.
        
        Args:
            collection_name: Name of the collection to query
            filter_doc: Query filter document (defaults to empty filter)
            projection: Fields to include/exclude in results
            sort: Sort specification as list of (field, direction) tuples
            **kwargs: Additional query options (limit, skip, etc.)
            
        Returns:
            QueryResult: Standardized result containing document or None
        """
        start_time = time.perf_counter()
        filter_doc = filter_doc or {}
        
        with database_error_context(
            operation="find_one",
            database=self.config.database_name,
            collection=collection_name
        ):
            collection = self.get_collection(collection_name)
            
            # Build query options
            find_kwargs = {}
            if projection:
                find_kwargs['projection'] = projection
            if sort:
                find_kwargs['sort'] = sort
            
            # Add additional options
            find_kwargs.update(kwargs)
            
            # Execute query
            document = collection.find_one(filter_doc, **find_kwargs)
            
            execution_time = (time.perf_counter() - start_time) * 1000
            
            logger.debug(
                "Find one operation completed",
                collection_name=collection_name,
                filter_doc=filter_doc,
                found=document is not None,
                execution_time_ms=execution_time
            )
            
            return QueryResult(
                success=True,
                data=document,
                count=1 if document else 0,
                execution_time_ms=execution_time,
                operation="find_one"
            )
    
    @with_database_retry(max_attempts=3, min_wait=1.0, max_wait=5.0)
    def find_many(
        self,
        collection_name: str,
        filter_doc: Optional[Dict[str, Any]] = None,
        projection: Optional[Dict[str, Any]] = None,
        sort: Optional[List[Tuple[str, int]]] = None,
        limit: Optional[int] = None,
        skip: Optional[int] = None,
        batch_size: Optional[int] = None,
        **kwargs
    ) -> QueryResult:
        """
        Find multiple documents matching the filter criteria.
        
        Implements identical query patterns from Node.js find() operations
        with cursor management and batch processing for optimal performance.
        
        Args:
            collection_name: Name of the collection to query
            filter_doc: Query filter document (defaults to empty filter)
            projection: Fields to include/exclude in results
            sort: Sort specification as list of (field, direction) tuples
            limit: Maximum number of documents to return
            skip: Number of documents to skip
            batch_size: Cursor batch size for memory optimization
            **kwargs: Additional query options
            
        Returns:
            QueryResult: Standardized result containing document list and count
        """
        start_time = time.perf_counter()
        filter_doc = filter_doc or {}
        
        with database_error_context(
            operation="find_many",
            database=self.config.database_name,
            collection=collection_name
        ):
            collection = self.get_collection(collection_name)
            
            # Build query options
            find_kwargs = {}
            if projection:
                find_kwargs['projection'] = projection
            if sort:
                find_kwargs['sort'] = sort
            if limit:
                find_kwargs['limit'] = limit
            if skip:
                find_kwargs['skip'] = skip
            if batch_size:
                find_kwargs['batch_size'] = batch_size
            else:
                find_kwargs['batch_size'] = self.config.default_batch_size
            
            # Add additional options
            find_kwargs.update(kwargs)
            
            # Execute query and convert cursor to list
            cursor = collection.find(filter_doc, **find_kwargs)
            documents = list(cursor)
            
            execution_time = (time.perf_counter() - start_time) * 1000
            
            logger.debug(
                "Find many operation completed",
                collection_name=collection_name,
                filter_doc=filter_doc,
                document_count=len(documents),
                execution_time_ms=execution_time
            )
            
            return QueryResult(
                success=True,
                data=documents,
                count=len(documents),
                execution_time_ms=execution_time,
                operation="find_many"
            )
    
    @with_database_retry(max_attempts=3, min_wait=1.0, max_wait=5.0)
    def insert_one(
        self,
        collection_name: str,
        document: Dict[str, Any],
        **kwargs
    ) -> QueryResult:
        """
        Insert a single document into the collection.
        
        Implements identical insert patterns from Node.js insertOne() operations
        with automatic ObjectId generation and write concern validation.
        
        Args:
            collection_name: Name of the collection for insertion
            document: Document to insert
            **kwargs: Additional insert options (write concern, etc.)
            
        Returns:
            QueryResult: Result containing inserted document ID and status
        """
        start_time = time.perf_counter()
        
        with database_error_context(
            operation="insert_one",
            database=self.config.database_name,
            collection=collection_name
        ):
            collection = self.get_collection(collection_name)
            
            # Ensure document has required fields
            if '_id' not in document:
                document['_id'] = ObjectId()
            
            # Add timestamps if not present
            current_time = datetime.now(timezone.utc)
            if 'createdAt' not in document:
                document['createdAt'] = current_time
            if 'updatedAt' not in document:
                document['updatedAt'] = current_time
            
            # Execute insert operation
            result = collection.insert_one(document, **kwargs)
            
            execution_time = (time.perf_counter() - start_time) * 1000
            
            logger.debug(
                "Insert one operation completed",
                collection_name=collection_name,
                inserted_id=str(result.inserted_id),
                acknowledged=result.acknowledged,
                execution_time_ms=execution_time
            )
            
            return QueryResult(
                success=True,
                inserted_id=result.inserted_id,
                acknowledged=result.acknowledged,
                execution_time_ms=execution_time,
                operation="insert_one"
            )
    
    @with_database_retry(max_attempts=3, min_wait=1.0, max_wait=5.0)
    def insert_many(
        self,
        collection_name: str,
        documents: List[Dict[str, Any]],
        ordered: bool = True,
        **kwargs
    ) -> QueryResult:
        """
        Insert multiple documents into the collection.
        
        Implements identical bulk insert patterns from Node.js insertMany() operations
        with batch processing and comprehensive error handling for partial failures.
        
        Args:
            collection_name: Name of the collection for insertion
            documents: List of documents to insert
            ordered: Whether to perform ordered or unordered insert
            **kwargs: Additional insert options
            
        Returns:
            QueryResult: Result containing inserted IDs and operation status
        """
        start_time = time.perf_counter()
        
        if not documents:
            return QueryResult(
                success=True,
                data=[],
                count=0,
                execution_time_ms=0,
                operation="insert_many"
            )
        
        with database_error_context(
            operation="insert_many",
            database=self.config.database_name,
            collection=collection_name
        ):
            collection = self.get_collection(collection_name)
            
            # Process documents for insertion
            current_time = datetime.now(timezone.utc)
            processed_docs = []
            
            for doc in documents:
                doc_copy = doc.copy()
                
                # Ensure document has required fields
                if '_id' not in doc_copy:
                    doc_copy['_id'] = ObjectId()
                
                # Add timestamps if not present
                if 'createdAt' not in doc_copy:
                    doc_copy['createdAt'] = current_time
                if 'updatedAt' not in doc_copy:
                    doc_copy['updatedAt'] = current_time
                
                processed_docs.append(doc_copy)
            
            # Execute bulk insert operation
            result = collection.insert_many(processed_docs, ordered=ordered, **kwargs)
            
            execution_time = (time.perf_counter() - start_time) * 1000
            
            logger.debug(
                "Insert many operation completed",
                collection_name=collection_name,
                document_count=len(result.inserted_ids),
                acknowledged=result.acknowledged,
                execution_time_ms=execution_time
            )
            
            return QueryResult(
                success=True,
                data=[str(oid) for oid in result.inserted_ids],
                count=len(result.inserted_ids),
                acknowledged=result.acknowledged,
                execution_time_ms=execution_time,
                operation="insert_many"
            )
    
    @with_database_retry(max_attempts=3, min_wait=1.0, max_wait=5.0)
    def update_one(
        self,
        collection_name: str,
        filter_doc: Dict[str, Any],
        update_doc: Dict[str, Any],
        upsert: bool = False,
        **kwargs
    ) -> QueryResult:
        """
        Update a single document matching the filter criteria.
        
        Implements identical update patterns from Node.js updateOne() operations
        with automatic timestamp management and comprehensive result tracking.
        
        Args:
            collection_name: Name of the collection to update
            filter_doc: Filter criteria to match documents
            update_doc: Update operations to apply
            upsert: Whether to insert if no document matches
            **kwargs: Additional update options
            
        Returns:
            QueryResult: Result containing update statistics and operation status
        """
        start_time = time.perf_counter()
        
        with database_error_context(
            operation="update_one",
            database=self.config.database_name,
            collection=collection_name
        ):
            collection = self.get_collection(collection_name)
            
            # Add automatic timestamp update
            if '$set' in update_doc:
                update_doc['$set']['updatedAt'] = datetime.now(timezone.utc)
            elif '$inc' in update_doc or '$push' in update_doc or '$pull' in update_doc:
                if '$set' not in update_doc:
                    update_doc['$set'] = {}
                update_doc['$set']['updatedAt'] = datetime.now(timezone.utc)
            else:
                # If no update operators, treat as $set operation
                update_doc = {'$set': {**update_doc, 'updatedAt': datetime.now(timezone.utc)}}
            
            # Execute update operation
            result = collection.update_one(filter_doc, update_doc, upsert=upsert, **kwargs)
            
            execution_time = (time.perf_counter() - start_time) * 1000
            
            logger.debug(
                "Update one operation completed",
                collection_name=collection_name,
                matched_count=result.matched_count,
                modified_count=result.modified_count,
                upserted_id=str(result.upserted_id) if result.upserted_id else None,
                acknowledged=result.acknowledged,
                execution_time_ms=execution_time
            )
            
            return QueryResult(
                success=True,
                matched_count=result.matched_count,
                modified_count=result.modified_count,
                upserted_id=result.upserted_id,
                acknowledged=result.acknowledged,
                execution_time_ms=execution_time,
                operation="update_one"
            )
    
    @with_database_retry(max_attempts=3, min_wait=1.0, max_wait=5.0)
    def update_many(
        self,
        collection_name: str,
        filter_doc: Dict[str, Any],
        update_doc: Dict[str, Any],
        upsert: bool = False,
        **kwargs
    ) -> QueryResult:
        """
        Update multiple documents matching the filter criteria.
        
        Implements identical bulk update patterns from Node.js updateMany() operations
        with batch processing and comprehensive statistics tracking.
        
        Args:
            collection_name: Name of the collection to update
            filter_doc: Filter criteria to match documents
            update_doc: Update operations to apply
            upsert: Whether to insert if no documents match
            **kwargs: Additional update options
            
        Returns:
            QueryResult: Result containing update statistics and operation status
        """
        start_time = time.perf_counter()
        
        with database_error_context(
            operation="update_many",
            database=self.config.database_name,
            collection=collection_name
        ):
            collection = self.get_collection(collection_name)
            
            # Add automatic timestamp update
            if '$set' in update_doc:
                update_doc['$set']['updatedAt'] = datetime.now(timezone.utc)
            elif '$inc' in update_doc or '$push' in update_doc or '$pull' in update_doc:
                if '$set' not in update_doc:
                    update_doc['$set'] = {}
                update_doc['$set']['updatedAt'] = datetime.now(timezone.utc)
            else:
                # If no update operators, treat as $set operation
                update_doc = {'$set': {**update_doc, 'updatedAt': datetime.now(timezone.utc)}}
            
            # Execute bulk update operation
            result = collection.update_many(filter_doc, update_doc, upsert=upsert, **kwargs)
            
            execution_time = (time.perf_counter() - start_time) * 1000
            
            logger.debug(
                "Update many operation completed",
                collection_name=collection_name,
                matched_count=result.matched_count,
                modified_count=result.modified_count,
                upserted_id=str(result.upserted_id) if result.upserted_id else None,
                acknowledged=result.acknowledged,
                execution_time_ms=execution_time
            )
            
            return QueryResult(
                success=True,
                matched_count=result.matched_count,
                modified_count=result.modified_count,
                upserted_id=result.upserted_id,
                acknowledged=result.acknowledged,
                execution_time_ms=execution_time,
                operation="update_many"
            )
    
    @with_database_retry(max_attempts=3, min_wait=1.0, max_wait=5.0)
    def delete_one(
        self,
        collection_name: str,
        filter_doc: Dict[str, Any],
        **kwargs
    ) -> QueryResult:
        """
        Delete a single document matching the filter criteria.
        
        Implements identical delete patterns from Node.js deleteOne() operations
        with comprehensive result tracking and soft delete support.
        
        Args:
            collection_name: Name of the collection to delete from
            filter_doc: Filter criteria to match document for deletion
            **kwargs: Additional delete options
            
        Returns:
            QueryResult: Result containing deletion statistics and operation status
        """
        start_time = time.perf_counter()
        
        with database_error_context(
            operation="delete_one",
            database=self.config.database_name,
            collection=collection_name
        ):
            collection = self.get_collection(collection_name)
            
            # Execute delete operation
            result = collection.delete_one(filter_doc, **kwargs)
            
            execution_time = (time.perf_counter() - start_time) * 1000
            
            logger.debug(
                "Delete one operation completed",
                collection_name=collection_name,
                deleted_count=result.deleted_count,
                acknowledged=result.acknowledged,
                execution_time_ms=execution_time
            )
            
            return QueryResult(
                success=True,
                deleted_count=result.deleted_count,
                acknowledged=result.acknowledged,
                execution_time_ms=execution_time,
                operation="delete_one"
            )
    
    @with_database_retry(max_attempts=3, min_wait=1.0, max_wait=5.0)
    def delete_many(
        self,
        collection_name: str,
        filter_doc: Dict[str, Any],
        **kwargs
    ) -> QueryResult:
        """
        Delete multiple documents matching the filter criteria.
        
        Implements identical bulk delete patterns from Node.js deleteMany() operations
        with batch processing and comprehensive deletion tracking.
        
        Args:
            collection_name: Name of the collection to delete from
            filter_doc: Filter criteria to match documents for deletion
            **kwargs: Additional delete options
            
        Returns:
            QueryResult: Result containing deletion statistics and operation status
        """
        start_time = time.perf_counter()
        
        with database_error_context(
            operation="delete_many",
            database=self.config.database_name,
            collection=collection_name
        ):
            collection = self.get_collection(collection_name)
            
            # Execute bulk delete operation
            result = collection.delete_many(filter_doc, **kwargs)
            
            execution_time = (time.perf_counter() - start_time) * 1000
            
            logger.debug(
                "Delete many operation completed",
                collection_name=collection_name,
                deleted_count=result.deleted_count,
                acknowledged=result.acknowledged,
                execution_time_ms=execution_time
            )
            
            return QueryResult(
                success=True,
                deleted_count=result.deleted_count,
                acknowledged=result.acknowledged,
                execution_time_ms=execution_time,
                operation="delete_many"
            )
    
    @with_database_retry(max_attempts=3, min_wait=1.0, max_wait=5.0)
    def count_documents(
        self,
        collection_name: str,
        filter_doc: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> QueryResult:
        """
        Count documents matching the filter criteria.
        
        Implements identical count patterns from Node.js countDocuments() operations
        with performance optimization for large collections.
        
        Args:
            collection_name: Name of the collection to count
            filter_doc: Filter criteria to match documents (defaults to empty filter)
            **kwargs: Additional count options (limit, skip, etc.)
            
        Returns:
            QueryResult: Result containing document count and operation status
        """
        start_time = time.perf_counter()
        filter_doc = filter_doc or {}
        
        with database_error_context(
            operation="count_documents",
            database=self.config.database_name,
            collection=collection_name
        ):
            collection = self.get_collection(collection_name)
            
            # Execute count operation
            count = collection.count_documents(filter_doc, **kwargs)
            
            execution_time = (time.perf_counter() - start_time) * 1000
            
            logger.debug(
                "Count documents operation completed",
                collection_name=collection_name,
                filter_doc=filter_doc,
                count=count,
                execution_time_ms=execution_time
            )
            
            return QueryResult(
                success=True,
                count=count,
                execution_time_ms=execution_time,
                operation="count_documents"
            )
    
    @with_database_retry(max_attempts=3, min_wait=1.0, max_wait=5.0)
    def aggregate(
        self,
        collection_name: str,
        pipeline: List[Dict[str, Any]],
        **kwargs
    ) -> QueryResult:
        """
        Execute aggregation pipeline on the collection.
        
        Implements identical aggregation patterns from Node.js aggregate() operations
        with cursor management and comprehensive result processing.
        
        Args:
            collection_name: Name of the collection to aggregate
            pipeline: Aggregation pipeline stages
            **kwargs: Additional aggregation options (batch size, etc.)
            
        Returns:
            QueryResult: Result containing aggregation results and operation status
        """
        start_time = time.perf_counter()
        
        with database_error_context(
            operation="aggregate",
            database=self.config.database_name,
            collection=collection_name
        ):
            collection = self.get_collection(collection_name)
            
            # Set default batch size if not provided
            if 'batchSize' not in kwargs:
                kwargs['batchSize'] = self.config.default_batch_size
            
            # Execute aggregation pipeline
            cursor = collection.aggregate(pipeline, **kwargs)
            results = list(cursor)
            
            execution_time = (time.perf_counter() - start_time) * 1000
            
            logger.debug(
                "Aggregation operation completed",
                collection_name=collection_name,
                pipeline_stages=len(pipeline),
                result_count=len(results),
                execution_time_ms=execution_time
            )
            
            return QueryResult(
                success=True,
                data=results,
                count=len(results),
                execution_time_ms=execution_time,
                operation="aggregate"
            )
    
    @contextmanager
    def transaction(
        self,
        read_concern: Optional[str] = None,
        write_concern: Optional[Dict[str, Any]] = None,
        read_preference: Optional[str] = None,
        max_commit_time_ms: Optional[int] = None
    ) -> Iterator[ClientSession]:
        """
        Context manager for MongoDB transactions with comprehensive error handling.
        
        Implements ACID transaction support per Section 4.2.2 state management
        with automatic retry, rollback on error, and monitoring integration.
        
        Args:
            read_concern: Transaction read concern level
            write_concern: Transaction write concern configuration
            read_preference: Transaction read preference
            max_commit_time_ms: Maximum commit timeout
            
        Yields:
            ClientSession: MongoDB session for transaction operations
            
        Raises:
            DatabaseTransactionError: If transaction fails or times out
        """
        if not self._initialized:
            self.initialize()
        
        session = self.client.start_session()
        transaction_id = str(ObjectId())
        
        # Configure transaction options
        transaction_options = {}
        if read_concern:
            read_concern_map = {
                'local': ReadConcern.LOCAL,
                'available': ReadConcern.AVAILABLE,
                'majority': ReadConcern.MAJORITY,
                'linearizable': ReadConcern.LINEARIZABLE,
                'snapshot': ReadConcern.SNAPSHOT
            }
            transaction_options['read_concern'] = read_concern_map.get(
                read_concern, 
                ReadConcern.MAJORITY
            )
        
        if write_concern:
            transaction_options['write_concern'] = WriteConcern(**write_concern)
        
        if read_preference:
            read_pref_map = {
                'primary': ReadPreference.PRIMARY,
                'primaryPreferred': ReadPreference.PRIMARY_PREFERRED,
                'secondary': ReadPreference.SECONDARY,
                'secondaryPreferred': ReadPreference.SECONDARY_PREFERRED,
                'nearest': ReadPreference.NEAREST
            }
            transaction_options['read_preference'] = read_pref_map.get(
                read_preference,
                ReadPreference.PRIMARY
            )
        
        if max_commit_time_ms:
            transaction_options['max_commit_time_ms'] = max_commit_time_ms
        
        try:
            # Start transaction with monitoring
            with monitor_transaction(
                self._monitoring_components['metrics'] if self._monitoring_components else None,
                self.config.database_name
            ):
                session.start_transaction(**transaction_options)
                
                logger.debug(
                    "Transaction started",
                    transaction_id=transaction_id,
                    database_name=self.config.database_name,
                    options=transaction_options
                )
                
                yield session
                
                # Commit transaction
                session.commit_transaction()
                
                logger.debug(
                    "Transaction committed successfully",
                    transaction_id=transaction_id,
                    database_name=self.config.database_name
                )
        
        except Exception as e:
            try:
                # Abort transaction on error
                session.abort_transaction()
                
                logger.warning(
                    "Transaction aborted due to error",
                    transaction_id=transaction_id,
                    database_name=self.config.database_name,
                    error=str(e)
                )
                
            except Exception as abort_error:
                logger.error(
                    "Failed to abort transaction",
                    transaction_id=transaction_id,
                    database_name=self.config.database_name,
                    abort_error=str(abort_error),
                    original_error=str(e)
                )
            
            # Convert to transaction error
            raise DatabaseTransactionError(
                message=f"Transaction failed: {str(e)}",
                transaction_id=transaction_id,
                database=self.config.database_name,
                operation="transaction",
                original_error=e
            )
        
        finally:
            try:
                session.end_session()
            except Exception as e:
                logger.error(
                    "Error ending transaction session",
                    transaction_id=transaction_id,
                    error=str(e)
                )
    
    @with_database_retry(max_attempts=3, min_wait=1.0, max_wait=5.0)
    def bulk_write(
        self,
        collection_name: str,
        operations: List[Dict[str, Any]],
        ordered: bool = True,
        **kwargs
    ) -> QueryResult:
        """
        Execute bulk write operations with comprehensive error handling.
        
        Implements batch processing for mixed insert, update, and delete operations
        with detailed result tracking and performance optimization.
        
        Args:
            collection_name: Name of the collection for bulk operations
            operations: List of bulk operation specifications
            ordered: Whether to execute operations in order
            **kwargs: Additional bulk write options
            
        Returns:
            QueryResult: Result containing bulk operation statistics
        """
        start_time = time.perf_counter()
        
        with database_error_context(
            operation="bulk_write",
            database=self.config.database_name,
            collection=collection_name
        ):
            collection = self.get_collection(collection_name)
            
            # Convert operation specifications to PyMongo operations
            pymongo_operations = []
            current_time = datetime.now(timezone.utc)
            
            for op in operations:
                if op['operation'] == 'insertOne':
                    doc = op['document'].copy()
                    if '_id' not in doc:
                        doc['_id'] = ObjectId()
                    if 'createdAt' not in doc:
                        doc['createdAt'] = current_time
                    if 'updatedAt' not in doc:
                        doc['updatedAt'] = current_time
                    
                    pymongo_operations.append(InsertOne(doc))
                
                elif op['operation'] == 'updateOne':
                    filter_doc = op['filter']
                    update_doc = op['update'].copy()
                    
                    # Add automatic timestamp update
                    if '$set' in update_doc:
                        update_doc['$set']['updatedAt'] = current_time
                    elif '$inc' in update_doc or '$push' in update_doc or '$pull' in update_doc:
                        if '$set' not in update_doc:
                            update_doc['$set'] = {}
                        update_doc['$set']['updatedAt'] = current_time
                    
                    pymongo_operations.append(UpdateOne(
                        filter_doc,
                        update_doc,
                        upsert=op.get('upsert', False)
                    ))
                
                elif op['operation'] == 'updateMany':
                    filter_doc = op['filter']
                    update_doc = op['update'].copy()
                    
                    # Add automatic timestamp update
                    if '$set' in update_doc:
                        update_doc['$set']['updatedAt'] = current_time
                    elif '$inc' in update_doc or '$push' in update_doc or '$pull' in update_doc:
                        if '$set' not in update_doc:
                            update_doc['$set'] = {}
                        update_doc['$set']['updatedAt'] = current_time
                    
                    pymongo_operations.append(UpdateMany(
                        filter_doc,
                        update_doc,
                        upsert=op.get('upsert', False)
                    ))
                
                elif op['operation'] == 'deleteOne':
                    pymongo_operations.append(DeleteOne(op['filter']))
                
                elif op['operation'] == 'deleteMany':
                    pymongo_operations.append(DeleteMany(op['filter']))
                
                elif op['operation'] == 'replaceOne':
                    doc = op['replacement'].copy()
                    if 'updatedAt' not in doc:
                        doc['updatedAt'] = current_time
                    
                    pymongo_operations.append(ReplaceOne(
                        op['filter'],
                        doc,
                        upsert=op.get('upsert', False)
                    ))
            
            # Execute bulk write operation
            result = collection.bulk_write(pymongo_operations, ordered=ordered, **kwargs)
            
            execution_time = (time.perf_counter() - start_time) * 1000
            
            logger.debug(
                "Bulk write operation completed",
                collection_name=collection_name,
                operation_count=len(operations),
                inserted_count=result.inserted_count,
                matched_count=result.matched_count,
                modified_count=result.modified_count,
                deleted_count=result.deleted_count,
                upserted_count=result.upserted_count,
                acknowledged=result.acknowledged,
                execution_time_ms=execution_time
            )
            
            # Prepare comprehensive result
            bulk_result = {
                'insertedCount': result.inserted_count,
                'matchedCount': result.matched_count,
                'modifiedCount': result.modified_count,
                'deletedCount': result.deleted_count,
                'upsertedCount': result.upserted_count,
                'upsertedIds': {str(k): str(v) for k, v in result.upserted_ids.items()},
                'insertedIds': [str(oid) for oid in getattr(result, 'inserted_ids', [])]
            }
            
            return QueryResult(
                success=True,
                data=bulk_result,
                count=len(operations),
                acknowledged=result.acknowledged,
                execution_time_ms=execution_time,
                operation="bulk_write"
            )
    
    def create_index(
        self,
        collection_name: str,
        keys: Union[str, List[Tuple[str, int]]],
        unique: bool = False,
        sparse: bool = False,
        background: bool = True,
        **kwargs
    ) -> QueryResult:
        """
        Create index on collection with performance optimization.
        
        Implements index creation patterns compatible with Node.js driver
        while providing comprehensive index management and monitoring.
        
        Args:
            collection_name: Name of the collection for index creation
            keys: Index key specification (string for single field or list of tuples)
            unique: Whether index should enforce uniqueness
            sparse: Whether index should be sparse
            background: Whether to create index in background
            **kwargs: Additional index options
            
        Returns:
            QueryResult: Result containing index creation status
        """
        start_time = time.perf_counter()
        
        with database_error_context(
            operation="create_index",
            database=self.config.database_name,
            collection=collection_name
        ):
            collection = self.get_collection(collection_name)
            
            # Normalize index key specification
            if isinstance(keys, str):
                index_keys = [(keys, ASCENDING)]
            else:
                index_keys = keys
            
            # Build index options
            index_options = {
                'unique': unique,
                'sparse': sparse,
                'background': background
            }
            index_options.update(kwargs)
            
            # Create index
            index_name = collection.create_index(index_keys, **index_options)
            
            execution_time = (time.perf_counter() - start_time) * 1000
            
            logger.info(
                "Index created successfully",
                collection_name=collection_name,
                index_name=index_name,
                index_keys=index_keys,
                unique=unique,
                sparse=sparse,
                execution_time_ms=execution_time
            )
            
            return QueryResult(
                success=True,
                data={'index_name': index_name, 'keys': index_keys},
                execution_time_ms=execution_time,
                operation="create_index"
            )
    
    def drop_index(
        self,
        collection_name: str,
        index_name: str
    ) -> QueryResult:
        """
        Drop index from collection.
        
        Args:
            collection_name: Name of the collection
            index_name: Name of the index to drop
            
        Returns:
            QueryResult: Result containing index drop status
        """
        start_time = time.perf_counter()
        
        with database_error_context(
            operation="drop_index",
            database=self.config.database_name,
            collection=collection_name
        ):
            collection = self.get_collection(collection_name)
            
            # Drop index
            collection.drop_index(index_name)
            
            execution_time = (time.perf_counter() - start_time) * 1000
            
            logger.info(
                "Index dropped successfully",
                collection_name=collection_name,
                index_name=index_name,
                execution_time_ms=execution_time
            )
            
            return QueryResult(
                success=True,
                data={'index_name': index_name},
                execution_time_ms=execution_time,
                operation="drop_index"
            )
    
    def get_health_status(self) -> Dict[str, Any]:
        """
        Get comprehensive database health status and connection information.
        
        Returns:
            Dict containing detailed health status, connection stats, and performance metrics
        """
        if not self._health_checker:
            return {
                'status': 'unknown',
                'error': 'Health checker not initialized',
                'timestamp': time.time()
            }
        
        try:
            # Check MongoDB health
            mongodb_health = self._health_checker.check_mongodb_health(
                self.client, 
                timeout=5.0
            )
            
            # Get connection statistics
            connection_stats = self._connection_stats.copy()
            
            # Get server information
            server_info = self._get_server_info()
            
            return {
                'status': mongodb_health['status'],
                'mongodb_health': mongodb_health,
                'connection_stats': connection_stats,
                'server_info': server_info,
                'config': {
                    'database_name': self.config.database_name,
                    'max_pool_size': self.config.max_pool_size,
                    'min_pool_size': self.config.min_pool_size
                },
                'timestamp': time.time()
            }
            
        except Exception as e:
            logger.error(f"Error getting database health status: {e}")
            return {
                'status': 'error',
                'error': str(e),
                'timestamp': time.time()
            }
    
    def close(self) -> None:
        """
        Close MongoDB client connection and cleanup resources.
        
        Properly closes all connections, ends sessions, and cleans up
        monitoring resources for graceful application shutdown.
        """
        with self._connection_lock:
            if self._client:
                try:
                    self._client.close()
                    self._connection_stats['active_connections'] = 0
                    
                    logger.info(
                        "MongoDB client connection closed",
                        database_name=self.config.database_name
                    )
                    
                except Exception as e:
                    logger.error(f"Error closing MongoDB client: {e}")
                
                finally:
                    self._client = None
                    self._database = None
                    self._initialized = False
    
    def __enter__(self):
        """Context manager entry."""
        self.initialize()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit with cleanup."""
        self.close()


def create_mongodb_client(config: Optional[MongoDBConfig] = None) -> MongoDBClient:
    """
    Factory function to create configured MongoDB client instance.
    
    Provides a centralized way to create MongoDB clients with proper configuration
    and monitoring setup for consistent database operations across the application.
    
    Args:
        config: MongoDB configuration (uses defaults if not provided)
        
    Returns:
        MongoDBClient: Configured and initialized MongoDB client
    """
    if config is None:
        config = MongoDBConfig()
    
    client = MongoDBClient(config)
    logger.info(
        "MongoDB client created",
        database_name=config.database_name,
        max_pool_size=config.max_pool_size
    )
    
    return client


def get_object_id(id_value: Union[str, ObjectId, None]) -> Optional[ObjectId]:
    """
    Convert string or ObjectId to valid ObjectId instance.
    
    Provides safe ObjectId conversion with proper error handling
    for consistent ID handling across database operations.
    
    Args:
        id_value: String representation or ObjectId instance
        
    Returns:
        ObjectId instance or None if conversion fails
    """
    if id_value is None:
        return None
    
    if isinstance(id_value, ObjectId):
        return id_value
    
    if isinstance(id_value, str):
        try:
            return ObjectId(id_value)
        except (InvalidId, TypeError):
            logger.warning(f"Invalid ObjectId format: {id_value}")
            return None
    
    logger.warning(f"Unsupported ID type: {type(id_value)}")
    return None


def serialize_for_json(data: Any) -> Any:
    """
    Serialize MongoDB documents for JSON compatibility.
    
    Converts BSON types (ObjectId, datetime, etc.) to JSON-serializable
    formats while preserving data structure and type information.
    
    Args:
        data: MongoDB document or data structure to serialize
        
    Returns:
        JSON-serializable data structure
    """
    try:
        return json.loads(json_util.dumps(data))
    except Exception as e:
        logger.error(f"Error serializing data for JSON: {e}")
        return None


# Export public interface
__all__ = [
    # Main client class
    'MongoDBClient',
    
    # Configuration and result classes
    'MongoDBConfig',
    'QueryResult',
    
    # Factory functions
    'create_mongodb_client',
    
    # Utility functions
    'get_object_id',
    'serialize_for_json',
    
    # Constants
    'DEFAULT_CONNECTION_TIMEOUT_MS',
    'DEFAULT_SERVER_SELECTION_TIMEOUT_MS',
    'DEFAULT_SOCKET_TIMEOUT_MS',
    'DEFAULT_MAX_POOL_SIZE',
    'DEFAULT_MIN_POOL_SIZE',
    'DEFAULT_MAX_IDLE_TIME_MS',
    'DEFAULT_WAIT_QUEUE_TIMEOUT_MS',
    'DEFAULT_TRANSACTION_TIMEOUT_SECONDS',
    'MAX_TRANSACTION_RETRY_ATTEMPTS',
    'DEFAULT_BATCH_SIZE',
    'MAX_BATCH_SIZE'
]