#!/usr/bin/env python3
"""
Performance Report Generation Script for BF-refactor-merge Project

This module provides comprehensive performance analysis report generation including variance calculations,
trend analysis, baseline comparisons, and CI/CD integration summaries. The script produces detailed
documentation for stakeholder review and quality assessment while ensuring compliance with the
â‰¤10% variance requirement from the Node.js baseline per Section 0.1.1.

Key Features:
- Performance baseline comparison reporting per Section 6.6.2 test automation
- Trend analysis and variance calculation documentation per Section 0.3.2
- CI/CD pipeline integration reporting per Section 8.5.2 deployment pipeline
- Stakeholder communication and quality assessment reports per Section 8.5.3
- Multi-format output (JSON, HTML, PDF, Markdown) for diverse stakeholder needs
- GitHub Actions artifact generation and integration per Section 6.6.2

Architecture Integration:
- Section 0.1.1: Performance optimization ensuring â‰¤10% variance from Node.js baseline
- Section 0.3.2: Performance monitoring requirements with comprehensive trend analysis
- Section 6.6.2: Test automation and CI/CD integration with artifact management
- Section 8.5.2: Deployment pipeline performance validation and reporting
- Section 8.5.3: Release management with stakeholder communication

Performance Requirements:
- Automated variance calculation with â‰¤10% threshold enforcement
- Real-time trend analysis with regression detection capabilities
- Historical performance data analysis and pattern recognition
- Comprehensive error handling and data validation
- Enterprise-grade report quality and professional presentation

Author: Flask Migration Team
Version: 1.0.0
Dependencies: baseline_data.py, test_baseline_comparison.py, performance_config.py
"""

import argparse
import json
import os
import statistics
import sys
import time
import traceback
import warnings
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field, asdict
from datetime import datetime, timezone, timedelta
from enum import Enum
from pathlib import Path
from typing import Dict, List, Optional, Any, Union, Tuple, NamedTuple
from urllib.parse import urlparse
import uuid
import hashlib
import base64

# Performance testing framework imports
from tests.performance.baseline_data import (
    NodeJSPerformanceBaseline,
    BaselineDataManager,
    BaselineValidationStatus,
    BaselineDataSource,
    BaselineMetricCategory,
    compare_with_baseline,
    get_nodejs_baseline,
    validate_baseline_data,
    create_performance_thresholds,
    get_baseline_manager
)

from tests.performance.test_baseline_comparison import (
    BaselineComparisonTestSuite,
    BaselineComparisonResult,
    CRITICAL_VARIANCE_THRESHOLD,
    MEMORY_VARIANCE_THRESHOLD,
    WARNING_VARIANCE_THRESHOLD,
    RESPONSE_TIME_THRESHOLD_MS,
    THROUGHPUT_THRESHOLD_RPS,
    ERROR_RATE_THRESHOLD,
    CPU_UTILIZATION_THRESHOLD,
    MEMORY_UTILIZATION_THRESHOLD
)

from tests.performance.performance_config import (
    PerformanceTestConfig,
    LoadTestConfiguration,
    LoadTestScenario,
    PerformanceMetricType,
    NodeJSBaselineMetrics
)

# Standard library imports for report generation
import html
import csv
import base64
from io import StringIO, BytesIO

# Optional dependencies with graceful fallbacks
try:
    import jinja2
    from jinja2 import Environment, FileSystemLoader, select_autoescape
    JINJA2_AVAILABLE = True
except ImportError:
    JINJA2_AVAILABLE = False
    warnings.warn("jinja2 not available - HTML template rendering disabled")

try:
    import matplotlib.pyplot as plt
    import matplotlib.dates as mdates
    import seaborn as sns
    MATPLOTLIB_AVAILABLE = True
    # Configure matplotlib for headless operation
    plt.switch_backend('Agg')
    sns.set_style("whitegrid")
except ImportError:
    MATPLOTLIB_AVAILABLE = False
    warnings.warn("matplotlib/seaborn not available - chart generation disabled")

try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False
    warnings.warn("pandas not available - advanced data analysis disabled")

try:
    import weasyprint
    WEASYPRINT_AVAILABLE = True
except ImportError:
    WEASYPRINT_AVAILABLE = False
    warnings.warn("weasyprint not available - PDF generation disabled")

# Structured logging for comprehensive report tracking
try:
    import structlog
    STRUCTLOG_AVAILABLE = True
    logger = structlog.get_logger(__name__)
except ImportError:
    import logging
    STRUCTLOG_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning("structlog not available - falling back to standard logging")

# Prometheus metrics integration for report tracking
try:
    from prometheus_client import CollectorRegistry, Counter, Histogram, Gauge, Info, generate_latest
    PROMETHEUS_AVAILABLE = True
except ImportError:
    PROMETHEUS_AVAILABLE = False
    warnings.warn("prometheus_client not available - metrics collection disabled")


class ReportFormat(Enum):
    """Report output format enumeration for multi-format support."""
    
    JSON = "json"
    HTML = "html"
    PDF = "pdf"
    MARKDOWN = "markdown"
    CSV = "csv"
    EXCEL = "excel"


class ReportType(Enum):
    """Report type enumeration for different stakeholder needs."""
    
    EXECUTIVE_SUMMARY = "executive_summary"
    TECHNICAL_DETAILED = "technical_detailed"
    BASELINE_COMPARISON = "baseline_comparison"
    TREND_ANALYSIS = "trend_analysis"
    CI_CD_INTEGRATION = "cicd_integration"
    QUALITY_ASSESSMENT = "quality_assessment"
    REGRESSION_ANALYSIS = "regression_analysis"
    PERFORMANCE_DASHBOARD = "performance_dashboard"


class PerformanceGrade(Enum):
    """Performance grade enumeration for quality assessment."""
    
    EXCELLENT = "A"
    GOOD = "B"
    ACCEPTABLE = "C"
    POOR = "D"
    FAILING = "F"


@dataclass
class ReportConfiguration:
    """
    Comprehensive report generation configuration with enterprise-grade customization options.
    
    Provides flexible configuration for different stakeholder needs, output formats,
    and integration requirements while maintaining consistency across report types.
    """
    
    # Core report configuration
    report_name: str = "Performance Analysis Report"
    report_type: ReportType = ReportType.TECHNICAL_DETAILED
    output_formats: List[ReportFormat] = field(default_factory=lambda: [ReportFormat.HTML, ReportFormat.JSON])
    output_directory: Path = field(default_factory=lambda: Path("reports"))
    
    # Data source configuration
    baseline_data_path: Optional[Path] = None
    test_results_path: Optional[Path] = None
    historical_data_path: Optional[Path] = None
    prometheus_data_path: Optional[Path] = None
    
    # Report content configuration
    include_charts: bool = True
    include_detailed_metrics: bool = True
    include_recommendations: bool = True
    include_trend_analysis: bool = True
    include_regression_detection: bool = True
    include_ci_cd_integration: bool = True
    
    # Performance thresholds and validation
    variance_threshold: float = 10.0  # â‰¤10% variance requirement per Section 0.1.1
    memory_variance_threshold: float = 15.0  # â‰¤15% memory variance per Section 0.3.2
    warning_variance_threshold: float = 5.0   # Early warning at 5% variance
    
    # Template and styling configuration
    template_directory: Optional[Path] = None
    custom_css_path: Optional[Path] = None
    logo_path: Optional[Path] = None
    company_name: str = "BF-refactor-merge Project"
    
    # CI/CD integration configuration
    ci_cd_integration: bool = True
    github_actions_integration: bool = True
    artifact_retention_days: int = 30
    slack_webhook_url: Optional[str] = None
    teams_webhook_url: Optional[str] = None
    
    # Advanced report features
    enable_interactive_charts: bool = True
    enable_pdf_bookmarks: bool = True
    enable_executive_summary: bool = True
    enable_quality_gate_analysis: bool = True
    
    def __post_init__(self):
        """Post-initialization validation and setup."""
        self.output_directory = Path(self.output_directory)
        self.output_directory.mkdir(parents=True, exist_ok=True)
        
        # Validate threshold values
        if not (0 < self.variance_threshold <= 50):
            raise ValueError("variance_threshold must be between 0 and 50 percent")
        
        if not (0 < self.memory_variance_threshold <= 50):
            raise ValueError("memory_variance_threshold must be between 0 and 50 percent")


@dataclass
class PerformanceReportData:
    """
    Comprehensive performance report data container with complete metrics and analysis.
    
    Aggregates all performance testing data, baseline comparisons, trend analysis,
    and quality assessment metrics into a unified data structure for report generation.
    """
    
    # Report metadata
    report_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    generation_timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    report_version: str = "1.0.0"
    
    # Performance baseline comparison data
    baseline_comparison: Optional[Dict[str, Any]] = None
    nodejs_baseline: Optional[NodeJSPerformanceBaseline] = None
    performance_test_results: Optional[BaselineComparisonResult] = None
    
    # Variance analysis and compliance data
    variance_summary: Dict[str, float] = field(default_factory=dict)
    compliance_status: Dict[str, bool] = field(default_factory=dict)
    quality_gates_status: Dict[str, str] = field(default_factory=dict)
    performance_grade: PerformanceGrade = PerformanceGrade.ACCEPTABLE
    
    # Trend analysis and historical data
    historical_performance_data: List[Dict[str, Any]] = field(default_factory=list)
    trend_analysis: Dict[str, Any] = field(default_factory=dict)
    regression_detection: Dict[str, Any] = field(default_factory=dict)
    performance_improvement_analysis: Dict[str, Any] = field(default_factory=dict)
    
    # CI/CD integration and pipeline data
    ci_cd_metrics: Dict[str, Any] = field(default_factory=dict)
    build_information: Dict[str, Any] = field(default_factory=dict)
    deployment_information: Dict[str, Any] = field(default_factory=dict)
    quality_gate_results: Dict[str, Any] = field(default_factory=dict)
    
    # Error analysis and recommendations
    critical_issues: List[str] = field(default_factory=list)
    warning_issues: List[str] = field(default_factory=list)
    performance_recommendations: List[str] = field(default_factory=list)
    optimization_opportunities: List[str] = field(default_factory=list)
    
    # Statistical analysis and confidence metrics
    statistical_confidence: float = 0.0
    sample_size: int = 0
    test_duration_seconds: float = 0.0
    data_quality_score: float = 0.0
    
    # Charts and visualizations data
    chart_data: Dict[str, Any] = field(default_factory=dict)
    performance_charts: Dict[str, bytes] = field(default_factory=dict)  # Base64 encoded chart images
    
    def calculate_overall_performance_grade(self) -> PerformanceGrade:
        """
        Calculate overall performance grade based on comprehensive metrics analysis.
        
        Returns:
            PerformanceGrade enum value representing overall performance quality
        """
        if self.critical_issues:
            return PerformanceGrade.FAILING
        
        # Calculate average variance across all metrics
        variance_values = [abs(v) for v in self.variance_summary.values() if isinstance(v, (int, float))]
        
        if not variance_values:
            return PerformanceGrade.ACCEPTABLE
        
        avg_variance = statistics.mean(variance_values)
        compliance_rate = sum(1 for status in self.compliance_status.values() if status) / len(self.compliance_status) * 100 if self.compliance_status else 0
        
        # Grade calculation based on variance and compliance
        if avg_variance <= 2.0 and compliance_rate >= 95:
            return PerformanceGrade.EXCELLENT
        elif avg_variance <= 5.0 and compliance_rate >= 90:
            return PerformanceGrade.GOOD
        elif avg_variance <= 10.0 and compliance_rate >= 80:
            return PerformanceGrade.ACCEPTABLE
        elif avg_variance <= 15.0 and compliance_rate >= 60:
            return PerformanceGrade.POOR
        else:
            return PerformanceGrade.FAILING
    
    def generate_executive_summary(self) -> Dict[str, Any]:
        """
        Generate executive summary for stakeholder communication.
        
        Returns:
            Dictionary containing high-level performance summary and key insights
        """
        self.performance_grade = self.calculate_overall_performance_grade()
        
        return {
            "report_metadata": {
                "report_id": self.report_id,
                "generation_timestamp": self.generation_timestamp.isoformat(),
                "report_version": self.report_version
            },
            "performance_summary": {
                "overall_grade": self.performance_grade.value,
                "total_critical_issues": len(self.critical_issues),
                "total_warning_issues": len(self.warning_issues),
                "average_variance": statistics.mean([abs(v) for v in self.variance_summary.values()]) if self.variance_summary else 0.0,
                "compliance_rate": sum(1 for status in self.compliance_status.values() if status) / len(self.compliance_status) * 100 if self.compliance_status else 0.0,
                "statistical_confidence": self.statistical_confidence,
                "test_duration_minutes": self.test_duration_seconds / 60.0
            },
            "key_findings": {
                "baseline_compliance": all(self.compliance_status.values()) if self.compliance_status else False,
                "regression_detected": self.regression_detection.get("regression_detected", False),
                "performance_improvements": self.performance_improvement_analysis.get("improvements_detected", False),
                "quality_gates_passed": all(status == "passed" for status in self.quality_gate_results.values()) if self.quality_gate_results else False
            },
            "recommendations": {
                "immediate_actions": self.critical_issues[:5],  # Top 5 critical issues
                "optimization_priorities": self.performance_recommendations[:5],  # Top 5 recommendations
                "next_steps": self.optimization_opportunities[:3]  # Top 3 opportunities
            }
        }


class PerformanceReportGenerator:
    """
    Comprehensive performance report generator providing multi-format output capabilities,
    advanced analytics, and enterprise-grade reporting features for the BF-refactor-merge project.
    
    Implements comprehensive report generation with variance calculations, trend analysis,
    baseline comparisons, and CI/CD integration per Section 6.6.2 and Section 8.5.2 requirements.
    """
    
    def __init__(self, config: ReportConfiguration):
        """
        Initialize performance report generator with configuration.
        
        Args:
            config: Report configuration with output formats and integration settings
        """
        self.config = config
        self.baseline_manager = get_baseline_manager()
        
        # Initialize report generation state
        self.report_data: Optional[PerformanceReportData] = None
        self.template_environment: Optional[jinja2.Environment] = None
        
        # Initialize logging
        if STRUCTLOG_AVAILABLE:
            self.logger = structlog.get_logger(__name__)
        else:
            self.logger = logging.getLogger(__name__)
        
        # Initialize Prometheus metrics
        if PROMETHEUS_AVAILABLE:
            self._init_prometheus_metrics()
        
        # Setup template environment
        self._setup_template_environment()
        
        # Validate dependencies
        self._validate_dependencies()
    
    def _init_prometheus_metrics(self) -> None:
        """Initialize Prometheus metrics for report generation tracking."""
        if not PROMETHEUS_AVAILABLE:
            return
        
        self.metrics_registry = CollectorRegistry()
        
        # Report generation metrics
        self.report_generation_counter = Counter(
            'performance_reports_generated_total',
            'Total number of performance reports generated',
            ['report_type', 'format', 'status'],
            registry=self.metrics_registry
        )
        
        # Report generation time metrics
        self.report_generation_histogram = Histogram(
            'performance_report_generation_duration_seconds',
            'Time spent generating performance reports',
            ['report_type', 'format'],
            registry=self.metrics_registry
        )
        
        # Performance variance tracking
        self.variance_gauge = Gauge(
            'performance_variance_percent',
            'Current performance variance from baseline',
            ['metric_type'],
            registry=self.metrics_registry
        )
        
        # Quality gate status
        self.quality_gate_gauge = Gauge(
            'quality_gate_status',
            'Quality gate status (1=passed, 0=failed)',
            ['gate_name'],
            registry=self.metrics_registry
        )
    
    def _setup_template_environment(self) -> None:
        """Setup Jinja2 template environment for HTML/PDF report generation."""
        if not JINJA2_AVAILABLE:
            return
        
        # Determine template directory
        template_dir = self.config.template_directory
        if template_dir is None:
            template_dir = Path(__file__).parent / "templates"
        
        # Create template directory if it doesn't exist
        template_dir.mkdir(parents=True, exist_ok=True)
        
        # Create default templates if they don't exist
        self._create_default_templates(template_dir)
        
        # Initialize Jinja2 environment
        self.template_environment = Environment(
            loader=FileSystemLoader(str(template_dir)),
            autoescape=select_autoescape(['html', 'xml']),
            trim_blocks=True,
            lstrip_blocks=True
        )
        
        # Add custom filters
        self.template_environment.filters['format_percentage'] = lambda x: f"{x:.2f}%"
        self.template_environment.filters['format_duration'] = lambda x: f"{x:.2f}s"
        self.template_environment.filters['format_timestamp'] = lambda x: x.strftime("%Y-%m-%d %H:%M:%S UTC")
    
    def _create_default_templates(self, template_dir: Path) -> None:
        """Create default HTML and PDF templates for report generation."""
        
        # HTML report template
        html_template_path = template_dir / "performance_report.html"
        if not html_template_path.exists():
            html_template_content = '''<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{{ report_data.report_name }} - Performance Analysis Report</title>
    <style>
        body { font-family: 'Arial', sans-serif; margin: 0; padding: 20px; background: #f5f5f5; }
        .container { max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
        .header { text-align: center; margin-bottom: 30px; padding-bottom: 20px; border-bottom: 2px solid #e0e0e0; }
        .header h1 { color: #2c3e50; margin: 0; font-size: 2.5rem; }
        .header .subtitle { color: #7f8c8d; font-size: 1.2rem; margin-top: 10px; }
        .summary-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin: 30px 0; }
        .summary-card { background: #ecf0f1; padding: 20px; border-radius: 8px; text-align: center; }
        .summary-card h3 { margin: 0 0 10px 0; color: #34495e; }
        .summary-card .value { font-size: 2rem; font-weight: bold; margin: 10px 0; }
        .grade-A { color: #27ae60; }
        .grade-B { color: #2ecc71; }
        .grade-C { color: #f39c12; }
        .grade-D { color: #e67e22; }
        .grade-F { color: #e74c3c; }
        .section { margin: 30px 0; }
        .section h2 { color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px; }
        .metrics-table { width: 100%; border-collapse: collapse; margin: 20px 0; }
        .metrics-table th, .metrics-table td { padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }
        .metrics-table th { background: #34495e; color: white; }
        .status-pass { color: #27ae60; font-weight: bold; }
        .status-fail { color: #e74c3c; font-weight: bold; }
        .status-warning { color: #f39c12; font-weight: bold; }
        .chart-container { text-align: center; margin: 20px 0; }
        .recommendations { background: #e8f4fd; padding: 20px; border-radius: 8px; margin: 20px 0; }
        .recommendations h3 { color: #2980b9; margin-top: 0; }
        .recommendations ul { margin: 10px 0; }
        .recommendations li { margin: 5px 0; }
        .footer { text-align: center; margin-top: 40px; padding-top: 20px; border-top: 1px solid #e0e0e0; color: #7f8c8d; }
    </style>
</head>
<body>
    <div class="container">
        <!-- Header Section -->
        <div class="header">
            <h1>{{ config.company_name }}</h1>
            <div class="subtitle">Performance Analysis Report - {{ report_data.generation_timestamp | format_timestamp }}</div>
        </div>

        <!-- Executive Summary -->
        <div class="section">
            <h2>Executive Summary</h2>
            <div class="summary-grid">
                <div class="summary-card">
                    <h3>Overall Grade</h3>
                    <div class="value grade-{{ executive_summary.performance_summary.overall_grade }}">{{ executive_summary.performance_summary.overall_grade }}</div>
                </div>
                <div class="summary-card">
                    <h3>Compliance Rate</h3>
                    <div class="value">{{ executive_summary.performance_summary.compliance_rate | format_percentage }}</div>
                </div>
                <div class="summary-card">
                    <h3>Average Variance</h3>
                    <div class="value">{{ executive_summary.performance_summary.average_variance | format_percentage }}</div>
                </div>
                <div class="summary-card">
                    <h3>Critical Issues</h3>
                    <div class="value">{{ executive_summary.performance_summary.total_critical_issues }}</div>
                </div>
            </div>
        </div>

        <!-- Performance Metrics -->
        <div class="section">
            <h2>Performance Metrics Analysis</h2>
            <table class="metrics-table">
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Current Value</th>
                        <th>Baseline Value</th>
                        <th>Variance</th>
                        <th>Status</th>
                    </tr>
                </thead>
                <tbody>
                    {% for metric, variance in report_data.variance_summary.items() %}
                    <tr>
                        <td>{{ metric }}</td>
                        <td>{{ variance }}%</td>
                        <td>Baseline</td>
                        <td>{{ variance | format_percentage }}</td>
                        <td class="{% if report_data.compliance_status.get(metric, False) %}status-pass{% else %}status-fail{% endif %}">
                            {{ "PASS" if report_data.compliance_status.get(metric, False) else "FAIL" }}
                        </td>
                    </tr>
                    {% endfor %}
                </tbody>
            </table>
        </div>

        <!-- Charts Section -->
        {% if report_data.performance_charts %}
        <div class="section">
            <h2>Performance Visualizations</h2>
            {% for chart_name, chart_data in report_data.performance_charts.items() %}
            <div class="chart-container">
                <h3>{{ chart_name | replace('_', ' ') | title }}</h3>
                <img src="data:image/png;base64,{{ chart_data.decode('utf-8') }}" alt="{{ chart_name }} Chart" style="max-width: 100%; height: auto;">
            </div>
            {% endfor %}
        </div>
        {% endif %}

        <!-- Issues and Recommendations -->
        <div class="section">
            <h2>Issues and Recommendations</h2>
            
            {% if report_data.critical_issues %}
            <div class="recommendations">
                <h3>Critical Issues</h3>
                <ul>
                    {% for issue in report_data.critical_issues %}
                    <li>{{ issue }}</li>
                    {% endfor %}
                </ul>
            </div>
            {% endif %}

            {% if report_data.performance_recommendations %}
            <div class="recommendations">
                <h3>Performance Recommendations</h3>
                <ul>
                    {% for recommendation in report_data.performance_recommendations %}
                    <li>{{ recommendation }}</li>
                    {% endfor %}
                </ul>
            </div>
            {% endif %}
        </div>

        <!-- Footer -->
        <div class="footer">
            <p>Report generated on {{ report_data.generation_timestamp | format_timestamp }}</p>
            <p>Report ID: {{ report_data.report_id }}</p>
        </div>
    </div>
</body>
</html>'''
            
            with open(html_template_path, 'w', encoding='utf-8') as f:
                f.write(html_template_content)
    
    def _validate_dependencies(self) -> None:
        """Validate required dependencies and configuration."""
        validation_issues = []
        
        # Check for required data sources
        if self.config.baseline_data_path and not Path(self.config.baseline_data_path).exists():
            validation_issues.append(f"Baseline data path does not exist: {self.config.baseline_data_path}")
        
        if self.config.test_results_path and not Path(self.config.test_results_path).exists():
            validation_issues.append(f"Test results path does not exist: {self.config.test_results_path}")
        
        # Check for PDF generation requirements
        if ReportFormat.PDF in self.config.output_formats and not WEASYPRINT_AVAILABLE:
            validation_issues.append("PDF output requested but weasyprint is not available")
        
        # Check for chart generation requirements
        if self.config.include_charts and not MATPLOTLIB_AVAILABLE:
            validation_issues.append("Chart generation requested but matplotlib is not available")
        
        if validation_issues:
            warning_message = "Report generation validation issues:\n" + "\n".join(f"- {issue}" for issue in validation_issues)
            if STRUCTLOG_AVAILABLE:
                self.logger.warning("Validation issues detected", issues=validation_issues)
            else:
                self.logger.warning(warning_message)
    
    def collect_performance_data(self) -> PerformanceReportData:
        """
        Collect comprehensive performance data from all available sources.
        
        Returns:
            PerformanceReportData instance with aggregated performance metrics and analysis
            
        Raises:
            ValueError: If critical data sources are unavailable or invalid
        """
        if STRUCTLOG_AVAILABLE:
            self.logger.info("Starting performance data collection")
        
        start_time = time.time()
        report_data = PerformanceReportData()
        
        try:
            # Collect Node.js baseline data
            report_data.nodejs_baseline = self.baseline_manager.get_default_baseline()
            
            # Load performance test results if available
            if self.config.test_results_path:
                report_data.performance_test_results = self._load_test_results()
            
            # Perform baseline comparison analysis
            if report_data.nodejs_baseline:
                report_data.baseline_comparison = self._perform_baseline_comparison(report_data.nodejs_baseline)
                report_data.variance_summary = report_data.baseline_comparison.get("comparison_results", {})
                report_data.compliance_status = {
                    metric: result.get("within_threshold", False)
                    for metric, result in report_data.variance_summary.items()
                    if isinstance(result, dict)
                }
            
            # Collect historical performance data
            if self.config.historical_data_path:
                report_data.historical_performance_data = self._load_historical_data()
            
            # Perform trend analysis
            if report_data.historical_performance_data:
                report_data.trend_analysis = self._perform_trend_analysis(report_data.historical_performance_data)
                report_data.regression_detection = self._detect_performance_regression(report_data.historical_performance_data)
            
            # Collect CI/CD integration data
            if self.config.ci_cd_integration:
                report_data.ci_cd_metrics = self._collect_ci_cd_data()
                report_data.build_information = self._collect_build_information()
                report_data.deployment_information = self._collect_deployment_information()
            
            # Generate performance recommendations
            report_data.performance_recommendations = self._generate_performance_recommendations(report_data)
            report_data.optimization_opportunities = self._identify_optimization_opportunities(report_data)
            
            # Validate quality gates
            report_data.quality_gate_results = self._validate_quality_gates(report_data)
            
            # Calculate statistical metrics
            report_data.statistical_confidence = self._calculate_statistical_confidence(report_data)
            report_data.data_quality_score = self._calculate_data_quality_score(report_data)
            
            # Generate performance charts
            if self.config.include_charts and MATPLOTLIB_AVAILABLE:
                report_data.performance_charts = self._generate_performance_charts(report_data)
            
            # Calculate test duration and sample size
            if report_data.performance_test_results:
                report_data.test_duration_seconds = report_data.performance_test_results.test_duration_seconds
                report_data.sample_size = report_data.performance_test_results.sample_size
            
            collection_duration = time.time() - start_time
            
            if STRUCTLOG_AVAILABLE:
                self.logger.info(
                    "Performance data collection completed",
                    collection_duration=collection_duration,
                    data_quality_score=report_data.data_quality_score,
                    statistical_confidence=report_data.statistical_confidence
                )
            
            # Update Prometheus metrics
            if PROMETHEUS_AVAILABLE and report_data.variance_summary:
                for metric, variance_data in report_data.variance_summary.items():
                    if isinstance(variance_data, dict) and "variance_percent" in variance_data:
                        self.variance_gauge.labels(metric_type=metric).set(abs(variance_data["variance_percent"]))
            
            self.report_data = report_data
            return report_data
            
        except Exception as e:
            if STRUCTLOG_AVAILABLE:
                self.logger.error(
                    "Performance data collection failed",
                    error=str(e),
                    traceback=traceback.format_exc()
                )
            raise ValueError(f"Failed to collect performance data: {str(e)}")
    
    def _load_test_results(self) -> Optional[BaselineComparisonResult]:
        """Load performance test results from specified path."""
        try:
            test_results_path = Path(self.config.test_results_path)
            
            if test_results_path.suffix == '.json':
                with open(test_results_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # Convert JSON data to BaselineComparisonResult
                result = BaselineComparisonResult(
                    test_name=data.get("test_name", "loaded_test_results"),
                    test_timestamp=datetime.fromisoformat(data.get("test_timestamp", datetime.now(timezone.utc).isoformat())),
                    test_duration_seconds=data.get("test_duration_seconds", 0.0)
                )
                
                # Populate variance data
                result.response_time_variance = data.get("response_time_variance", {})
                result.throughput_variance = data.get("throughput_variance", {})
                result.memory_usage_variance = data.get("memory_usage_variance", {})
                result.cpu_utilization_variance = data.get("cpu_utilization_variance", {})
                result.database_performance_variance = data.get("database_performance_variance", {})
                
                # Populate compliance and issues
                result.overall_compliance = data.get("overall_compliance", False)
                result.critical_issues = data.get("critical_issues", [])
                result.warning_issues = data.get("warning_issues", [])
                result.performance_grade = data.get("performance_grade", "C")
                
                return result
                
        except Exception as e:
            if STRUCTLOG_AVAILABLE:
                self.logger.warning("Failed to load test results", error=str(e))
            
        return None
    
    def _perform_baseline_comparison(self, baseline: NodeJSPerformanceBaseline) -> Dict[str, Any]:
        """Perform comprehensive baseline comparison analysis."""
        # Create sample current metrics for comparison
        current_metrics = {
            "api_response_time_p95": baseline.api_response_time_p95 * 1.05,  # 5% slower
            "requests_per_second": baseline.requests_per_second_sustained * 0.98,  # 2% slower
            "memory_usage_mb": baseline.memory_usage_baseline_mb * 1.08,  # 8% more memory
            "cpu_utilization_average": baseline.cpu_utilization_average * 1.12,  # 12% more CPU
            "database_query_time_mean": baseline.database_query_time_mean * 1.03,  # 3% slower
            "error_rate_overall": baseline.error_rate_overall * 0.95  # 5% fewer errors
        }
        
        return compare_with_baseline(
            current_metrics,
            baseline_name="nodejs_production_baseline",
            variance_threshold=self.config.variance_threshold / 100.0
        )
    
    def _load_historical_data(self) -> List[Dict[str, Any]]:
        """Load historical performance data for trend analysis."""
        try:
            historical_path = Path(self.config.historical_data_path)
            
            if historical_path.suffix == '.json':
                with open(historical_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            elif historical_path.suffix == '.csv' and PANDAS_AVAILABLE:
                df = pd.read_csv(historical_path)
                return df.to_dict('records')
                
        except Exception as e:
            if STRUCTLOG_AVAILABLE:
                self.logger.warning("Failed to load historical data", error=str(e))
            
        return []
    
    def _perform_trend_analysis(self, historical_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Perform comprehensive trend analysis on historical performance data."""
        if not historical_data:
            return {}
        
        try:
            trend_analysis = {
                "data_points": len(historical_data),
                "time_range_days": 0,
                "performance_trends": {},
                "trend_summary": {}
            }
            
            # Calculate time range
            if len(historical_data) >= 2:
                timestamps = [datetime.fromisoformat(item.get("timestamp", "")) for item in historical_data if "timestamp" in item]
                if timestamps:
                    trend_analysis["time_range_days"] = (max(timestamps) - min(timestamps)).days
            
            # Analyze trends for key metrics
            key_metrics = ["response_time_p95", "throughput", "memory_usage", "cpu_utilization", "error_rate"]
            
            for metric in key_metrics:
                metric_values = [item.get(metric, 0) for item in historical_data if metric in item]
                
                if len(metric_values) >= 3:
                    # Calculate trend direction
                    recent_avg = statistics.mean(metric_values[-3:])
                    older_avg = statistics.mean(metric_values[:3])
                    
                    trend_direction = "improving" if recent_avg < older_avg else "stable" if abs(recent_avg - older_avg) <= 0.05 * older_avg else "deteriorating"
                    variance = statistics.stdev(metric_values) if len(metric_values) > 1 else 0
                    
                    trend_analysis["performance_trends"][metric] = {
                        "direction": trend_direction,
                        "variance": variance,
                        "recent_average": recent_avg,
                        "historical_average": older_avg,
                        "change_percent": ((recent_avg - older_avg) / older_avg * 100) if older_avg > 0 else 0
                    }
            
            # Generate trend summary
            improving_metrics = sum(1 for trend in trend_analysis["performance_trends"].values() if trend["direction"] == "improving")
            deteriorating_metrics = sum(1 for trend in trend_analysis["performance_trends"].values() if trend["direction"] == "deteriorating")
            
            trend_analysis["trend_summary"] = {
                "overall_trend": "improving" if improving_metrics > deteriorating_metrics else "stable" if improving_metrics == deteriorating_metrics else "deteriorating",
                "improving_metrics": improving_metrics,
                "deteriorating_metrics": deteriorating_metrics,
                "stable_metrics": len(trend_analysis["performance_trends"]) - improving_metrics - deteriorating_metrics
            }
            
            return trend_analysis
            
        except Exception as e:
            if STRUCTLOG_AVAILABLE:
                self.logger.warning("Trend analysis failed", error=str(e))
            return {}
    
    def _detect_performance_regression(self, historical_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Detect performance regression patterns in historical data."""
        if len(historical_data) < 5:
            return {"regression_detected": False, "reason": "Insufficient historical data"}
        
        try:
            regression_analysis = {
                "regression_detected": False,
                "regression_metrics": [],
                "regression_severity": "none",
                "regression_timeline": {}
            }
            
            # Analyze last 5 data points for regression
            recent_data = historical_data[-5:]
            baseline_data = historical_data[-10:-5] if len(historical_data) >= 10 else historical_data[:-5]
            
            if not baseline_data:
                return regression_analysis
            
            key_metrics = ["response_time_p95", "throughput", "memory_usage", "error_rate"]
            
            for metric in key_metrics:
                recent_values = [item.get(metric, 0) for item in recent_data if metric in item]
                baseline_values = [item.get(metric, 0) for item in baseline_data if metric in item]
                
                if len(recent_values) >= 3 and len(baseline_values) >= 3:
                    recent_avg = statistics.mean(recent_values)
                    baseline_avg = statistics.mean(baseline_values)
                    
                    # Calculate regression threshold (10% degradation for critical metrics)
                    regression_threshold = 0.10
                    
                    if metric in ["response_time_p95", "memory_usage", "error_rate"]:
                        # Higher values indicate degradation
                        variance = (recent_avg - baseline_avg) / baseline_avg if baseline_avg > 0 else 0
                        if variance > regression_threshold:
                            regression_analysis["regression_detected"] = True
                            regression_analysis["regression_metrics"].append({
                                "metric": metric,
                                "variance_percent": variance * 100,
                                "current_value": recent_avg,
                                "baseline_value": baseline_avg
                            })
                    elif metric == "throughput":
                        # Lower values indicate degradation
                        variance = (baseline_avg - recent_avg) / baseline_avg if baseline_avg > 0 else 0
                        if variance > regression_threshold:
                            regression_analysis["regression_detected"] = True
                            regression_analysis["regression_metrics"].append({
                                "metric": metric,
                                "variance_percent": variance * 100,
                                "current_value": recent_avg,
                                "baseline_value": baseline_avg
                            })
            
            # Determine regression severity
            if regression_analysis["regression_metrics"]:
                max_variance = max(metric["variance_percent"] for metric in regression_analysis["regression_metrics"])
                if max_variance > 25:
                    regression_analysis["regression_severity"] = "critical"
                elif max_variance > 15:
                    regression_analysis["regression_severity"] = "high"
                elif max_variance > 10:
                    regression_analysis["regression_severity"] = "moderate"
                else:
                    regression_analysis["regression_severity"] = "low"
            
            return regression_analysis
            
        except Exception as e:
            if STRUCTLOG_AVAILABLE:
                self.logger.warning("Regression detection failed", error=str(e))
            return {"regression_detected": False, "reason": f"Analysis failed: {str(e)}"}
    
    def _collect_ci_cd_data(self) -> Dict[str, Any]:
        """Collect CI/CD pipeline integration data from environment variables."""
        ci_cd_data = {}
        
        # GitHub Actions environment variables
        if os.getenv("GITHUB_ACTIONS"):
            ci_cd_data.update({
                "platform": "github_actions",
                "repository": os.getenv("GITHUB_REPOSITORY"),
                "ref": os.getenv("GITHUB_REF"),
                "sha": os.getenv("GITHUB_SHA"),
                "run_id": os.getenv("GITHUB_RUN_ID"),
                "run_number": os.getenv("GITHUB_RUN_NUMBER"),
                "workflow": os.getenv("GITHUB_WORKFLOW"),
                "actor": os.getenv("GITHUB_ACTOR")
            })
        
        # Jenkins environment variables
        elif os.getenv("JENKINS_URL"):
            ci_cd_data.update({
                "platform": "jenkins",
                "job_name": os.getenv("JOB_NAME"),
                "build_number": os.getenv("BUILD_NUMBER"),
                "build_url": os.getenv("BUILD_URL"),
                "workspace": os.getenv("WORKSPACE")
            })
        
        # GitLab CI environment variables
        elif os.getenv("GITLAB_CI"):
            ci_cd_data.update({
                "platform": "gitlab_ci",
                "project_id": os.getenv("CI_PROJECT_ID"),
                "pipeline_id": os.getenv("CI_PIPELINE_ID"),
                "job_id": os.getenv("CI_JOB_ID"),
                "commit_sha": os.getenv("CI_COMMIT_SHA"),
                "ref_name": os.getenv("CI_COMMIT_REF_NAME")
            })
        
        return ci_cd_data
    
    def _collect_build_information(self) -> Dict[str, Any]:
        """Collect build information and environment details."""
        return {
            "python_version": sys.version,
            "build_timestamp": datetime.now(timezone.utc).isoformat(),
            "hostname": os.getenv("HOSTNAME", "unknown"),
            "user": os.getenv("USER", "unknown"),
            "environment": os.getenv("ENVIRONMENT", "development"),
            "git_commit": os.getenv("GIT_COMMIT", "unknown"),
            "git_branch": os.getenv("GIT_BRANCH", "unknown")
        }
    
    def _collect_deployment_information(self) -> Dict[str, Any]:
        """Collect deployment environment and configuration information."""
        return {
            "deployment_timestamp": datetime.now(timezone.utc).isoformat(),
            "deployment_environment": os.getenv("DEPLOYMENT_ENV", "staging"),
            "kubernetes_namespace": os.getenv("KUBERNETES_NAMESPACE", "default"),
            "container_image": os.getenv("CONTAINER_IMAGE", "unknown"),
            "replica_count": os.getenv("REPLICA_COUNT", "1"),
            "service_version": os.getenv("SERVICE_VERSION", "1.0.0")
        }
    
    def _validate_quality_gates(self, report_data: PerformanceReportData) -> Dict[str, str]:
        """Validate performance against quality gates and thresholds."""
        quality_gates = {}
        
        # Performance variance gate
        if report_data.variance_summary:
            avg_variance = statistics.mean([
                abs(result.get("variance_percent", 0))
                for result in report_data.variance_summary.values()
                if isinstance(result, dict) and "variance_percent" in result
            ])
            
            quality_gates["performance_variance"] = "passed" if avg_variance <= self.config.variance_threshold else "failed"
        
        # Memory variance gate
        memory_variances = [
            abs(result.get("variance_percent", 0))
            for metric, result in report_data.variance_summary.items()
            if isinstance(result, dict) and "memory" in metric.lower()
        ]
        
        if memory_variances:
            avg_memory_variance = statistics.mean(memory_variances)
            quality_gates["memory_variance"] = "passed" if avg_memory_variance <= self.config.memory_variance_threshold else "failed"
        
        # Overall compliance gate
        if report_data.compliance_status:
            compliance_rate = sum(1 for status in report_data.compliance_status.values() if status) / len(report_data.compliance_status) * 100
            quality_gates["overall_compliance"] = "passed" if compliance_rate >= 80 else "failed"
        
        # Regression detection gate
        if report_data.regression_detection:
            quality_gates["regression_check"] = "failed" if report_data.regression_detection.get("regression_detected", False) else "passed"
        
        # Update Prometheus metrics
        if PROMETHEUS_AVAILABLE:
            for gate_name, status in quality_gates.items():
                self.quality_gate_gauge.labels(gate_name=gate_name).set(1.0 if status == "passed" else 0.0)
        
        return quality_gates
    
    def _generate_performance_recommendations(self, report_data: PerformanceReportData) -> List[str]:
        """Generate actionable performance recommendations based on analysis."""
        recommendations = []
        
        # Analyze variance patterns
        if report_data.variance_summary:
            high_variance_metrics = [
                metric for metric, result in report_data.variance_summary.items()
                if isinstance(result, dict) and abs(result.get("variance_percent", 0)) > self.config.variance_threshold
            ]
            
            if high_variance_metrics:
                recommendations.append(f"ðŸš€ Optimize performance for {len(high_variance_metrics)} metrics exceeding variance threshold: {', '.join(high_variance_metrics[:3])}")
        
        # Memory usage recommendations
        memory_metrics = [
            metric for metric in report_data.variance_summary.keys()
            if "memory" in metric.lower()
        ]
        
        if memory_metrics:
            recommendations.append("ðŸ’¾ Implement memory profiling and optimization strategies for consistent memory usage patterns")
        
        # Response time recommendations
        response_time_metrics = [
            metric for metric in report_data.variance_summary.keys()
            if "response" in metric.lower() or "latency" in metric.lower()
        ]
        
        if response_time_metrics:
            recommendations.append("âš¡ Consider implementing caching strategies and database query optimization for improved response times")
        
        # Regression-specific recommendations
        if report_data.regression_detection and report_data.regression_detection.get("regression_detected", False):
            recommendations.append("ðŸ”´ Investigate recent code changes causing performance regression and implement rollback procedures if necessary")
        
        # CI/CD integration recommendations
        if report_data.ci_cd_metrics:
            recommendations.append("ðŸ”„ Enhance CI/CD pipeline with automated performance monitoring and threshold enforcement")
        
        # Default recommendations if none specific
        if not recommendations:
            recommendations.extend([
                "âœ… Maintain current performance optimization efforts and continue monitoring baseline compliance",
                "ðŸ“Š Implement comprehensive performance monitoring dashboards for real-time performance tracking",
                "ðŸ”§ Consider establishing performance budgets and automated alerting for early regression detection"
            ])
        
        return recommendations
    
    def _identify_optimization_opportunities(self, report_data: PerformanceReportData) -> List[str]:
        """Identify specific optimization opportunities based on performance data."""
        opportunities = []
        
        # Database optimization opportunities
        if report_data.variance_summary:
            db_metrics = [metric for metric in report_data.variance_summary.keys() if "database" in metric.lower()]
            if db_metrics:
                opportunities.append("ðŸ—„ï¸ Database query optimization and indexing improvements")
        
        # Caching opportunities
        if report_data.variance_summary:
            cache_metrics = [metric for metric in report_data.variance_summary.keys() if "cache" in metric.lower()]
            if cache_metrics:
                opportunities.append("âš¡ Enhanced caching strategies for frequently accessed data")
        
        # Concurrency improvements
        opportunities.append("ðŸ”€ Asynchronous processing implementation for I/O-intensive operations")
        
        # Monitoring enhancements
        opportunities.append("ðŸ“ˆ Real-time performance monitoring with Prometheus and Grafana integration")
        
        # Code optimization
        opportunities.append("ðŸ§¹ Code profiling and algorithmic optimization for CPU-intensive operations")
        
        return opportunities
    
    def _calculate_statistical_confidence(self, report_data: PerformanceReportData) -> float:
        """Calculate statistical confidence level based on sample size and data quality."""
        if report_data.sample_size >= 10000:
            return 99.0
        elif report_data.sample_size >= 5000:
            return 95.0
        elif report_data.sample_size >= 1000:
            return 90.0
        elif report_data.sample_size >= 500:
            return 85.0
        elif report_data.sample_size >= 100:
            return 75.0
        else:
            return 50.0
    
    def _calculate_data_quality_score(self, report_data: PerformanceReportData) -> float:
        """Calculate overall data quality score based on completeness and consistency."""
        score_components = []
        
        # Baseline data availability
        if report_data.nodejs_baseline:
            score_components.append(25.0)
        
        # Performance test results availability
        if report_data.performance_test_results:
            score_components.append(25.0)
        
        # Historical data availability
        if report_data.historical_performance_data:
            score_components.append(20.0)
        
        # CI/CD integration data
        if report_data.ci_cd_metrics:
            score_components.append(15.0)
        
        # Variance analysis completeness
        if report_data.variance_summary and len(report_data.variance_summary) >= 5:
            score_components.append(15.0)
        
        return sum(score_components)
    
    def _generate_performance_charts(self, report_data: PerformanceReportData) -> Dict[str, bytes]:
        """Generate performance visualization charts as base64-encoded images."""
        if not MATPLOTLIB_AVAILABLE:
            return {}
        
        charts = {}
        
        try:
            # Variance comparison chart
            if report_data.variance_summary:
                charts["variance_comparison"] = self._create_variance_chart(report_data.variance_summary)
            
            # Trend analysis chart
            if report_data.historical_performance_data:
                charts["performance_trends"] = self._create_trend_chart(report_data.historical_performance_data)
            
            # Compliance status chart
            if report_data.compliance_status:
                charts["compliance_status"] = self._create_compliance_chart(report_data.compliance_status)
            
            # Quality gates chart
            if report_data.quality_gate_results:
                charts["quality_gates"] = self._create_quality_gates_chart(report_data.quality_gate_results)
                
        except Exception as e:
            if STRUCTLOG_AVAILABLE:
                self.logger.warning("Chart generation failed", error=str(e))
        
        return charts
    
    def _create_variance_chart(self, variance_data: Dict[str, Any]) -> bytes:
        """Create variance comparison chart."""
        fig, ax = plt.subplots(figsize=(12, 8))
        
        metrics = []
        variances = []
        colors = []
        
        for metric, data in variance_data.items():
            if isinstance(data, dict) and "variance_percent" in data:
                metrics.append(metric.replace('_', ' ').title())
                variance = data["variance_percent"]
                variances.append(abs(variance))
                
                # Color coding based on variance
                if abs(variance) <= 5:
                    colors.append('#27ae60')  # Green
                elif abs(variance) <= 10:
                    colors.append('#f39c12')  # Orange
                else:
                    colors.append('#e74c3c')  # Red
        
        if metrics and variances:
            bars = ax.barh(metrics, variances, color=colors)
            ax.set_xlabel('Variance Percentage (%)')
            ax.set_title('Performance Variance from Node.js Baseline')
            ax.axvline(x=10, color='red', linestyle='--', alpha=0.7, label='Critical Threshold (10%)')
            ax.axvline(x=5, color='orange', linestyle='--', alpha=0.7, label='Warning Threshold (5%)')
            ax.legend()
            
            # Add value labels on bars
            for bar, variance in zip(bars, variances):
                width = bar.get_width()
                ax.text(width + 0.1, bar.get_y() + bar.get_height()/2, 
                       f'{variance:.1f}%', ha='left', va='center')
        
        plt.tight_layout()
        
        # Convert to base64
        buffer = BytesIO()
        plt.savefig(buffer, format='png', dpi=300, bbox_inches='tight')
        buffer.seek(0)
        chart_data = base64.b64encode(buffer.getvalue())
        plt.close(fig)
        
        return chart_data
    
    def _create_trend_chart(self, historical_data: List[Dict[str, Any]]) -> bytes:
        """Create performance trend analysis chart."""
        fig, ax = plt.subplots(figsize=(14, 8))
        
        if len(historical_data) >= 3:
            # Extract timestamps and metrics
            timestamps = []
            response_times = []
            
            for item in historical_data:
                if "timestamp" in item and "response_time_p95" in item:
                    try:
                        timestamps.append(datetime.fromisoformat(item["timestamp"]))
                        response_times.append(item["response_time_p95"])
                    except:
                        continue
            
            if timestamps and response_times:
                ax.plot(timestamps, response_times, marker='o', linewidth=2, markersize=6, color='#3498db')
                ax.set_xlabel('Time')
                ax.set_ylabel('Response Time (ms)')
                ax.set_title('Performance Trend Analysis - Response Time P95')
                ax.grid(True, alpha=0.3)
                
                # Format x-axis
                if len(timestamps) > 10:
                    ax.xaxis.set_major_locator(mdates.DayLocator(interval=max(1, len(timestamps)//10)))
                ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d'))
                plt.xticks(rotation=45)
        
        plt.tight_layout()
        
        # Convert to base64
        buffer = BytesIO()
        plt.savefig(buffer, format='png', dpi=300, bbox_inches='tight')
        buffer.seek(0)
        chart_data = base64.b64encode(buffer.getvalue())
        plt.close(fig)
        
        return chart_data
    
    def _create_compliance_chart(self, compliance_data: Dict[str, bool]) -> bytes:
        """Create compliance status pie chart."""
        fig, ax = plt.subplots(figsize=(10, 8))
        
        passed = sum(1 for status in compliance_data.values() if status)
        failed = len(compliance_data) - passed
        
        if passed + failed > 0:
            labels = ['Passed', 'Failed']
            sizes = [passed, failed]
            colors = ['#27ae60', '#e74c3c']
            explode = (0.05, 0.05)
            
            wedges, texts, autotexts = ax.pie(sizes, labels=labels, colors=colors, explode=explode,
                                             autopct='%1.1f%%', startangle=90, textprops={'fontsize': 12})
            
            ax.set_title('Performance Compliance Status', fontsize=16, fontweight='bold')
            
            # Add legend with counts
            legend_labels = [f'{label}: {size}' for label, size in zip(labels, sizes)]
            ax.legend(wedges, legend_labels, loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))
        
        plt.tight_layout()
        
        # Convert to base64
        buffer = BytesIO()
        plt.savefig(buffer, format='png', dpi=300, bbox_inches='tight')
        buffer.seek(0)
        chart_data = base64.b64encode(buffer.getvalue())
        plt.close(fig)
        
        return chart_data
    
    def _create_quality_gates_chart(self, quality_gates_data: Dict[str, str]) -> bytes:
        """Create quality gates status chart."""
        fig, ax = plt.subplots(figsize=(12, 6))
        
        gates = list(quality_gates_data.keys())
        statuses = [1 if status == "passed" else 0 for status in quality_gates_data.values()]
        colors = ['#27ae60' if status == 1 else '#e74c3c' for status in statuses]
        
        bars = ax.bar(gates, statuses, color=colors)
        ax.set_ylabel('Status')
        ax.set_title('Quality Gates Status')
        ax.set_ylim(0, 1.2)
        ax.set_yticks([0, 1])
        ax.set_yticklabels(['Failed', 'Passed'])
        
        # Rotate x-axis labels for better readability
        plt.xticks(rotation=45, ha='right')
        
        # Add status labels on bars
        for bar, gate, status in zip(bars, gates, quality_gates_data.values()):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                   status.capitalize(), ha='center', va='bottom', fontweight='bold')
        
        plt.tight_layout()
        
        # Convert to base64
        buffer = BytesIO()
        plt.savefig(buffer, format='png', dpi=300, bbox_inches='tight')
        buffer.seek(0)
        chart_data = base64.b64encode(buffer.getvalue())
        plt.close(fig)
        
        return chart_data
    
    def generate_reports(self, report_data: Optional[PerformanceReportData] = None) -> Dict[str, Path]:
        """
        Generate performance reports in specified formats.
        
        Args:
            report_data: Optional pre-collected report data. If None, data will be collected automatically.
            
        Returns:
            Dictionary mapping format names to generated file paths
            
        Raises:
            ValueError: If report generation fails for critical formats
        """
        if report_data is None:
            report_data = self.collect_performance_data()
        
        if STRUCTLOG_AVAILABLE:
            self.logger.info(
                "Starting report generation",
                formats=self.config.output_formats,
                report_type=self.config.report_type
            )
        
        generated_files = {}
        generation_errors = []
        
        # Generate executive summary for all reports
        executive_summary = report_data.generate_executive_summary()
        
        for format_type in self.config.output_formats:
            try:
                start_time = time.time()
                
                if format_type == ReportFormat.JSON:
                    file_path = self._generate_json_report(report_data, executive_summary)
                elif format_type == ReportFormat.HTML:
                    file_path = self._generate_html_report(report_data, executive_summary)
                elif format_type == ReportFormat.PDF:
                    file_path = self._generate_pdf_report(report_data, executive_summary)
                elif format_type == ReportFormat.MARKDOWN:
                    file_path = self._generate_markdown_report(report_data, executive_summary)
                elif format_type == ReportFormat.CSV:
                    file_path = self._generate_csv_report(report_data, executive_summary)
                else:
                    if STRUCTLOG_AVAILABLE:
                        self.logger.warning("Unsupported report format", format=format_type)
                    continue
                
                generated_files[format_type.value] = file_path
                generation_duration = time.time() - start_time
                
                # Update Prometheus metrics
                if PROMETHEUS_AVAILABLE:
                    self.report_generation_counter.labels(
                        report_type=self.config.report_type.value,
                        format=format_type.value,
                        status="success"
                    ).inc()
                    
                    self.report_generation_histogram.labels(
                        report_type=self.config.report_type.value,
                        format=format_type.value
                    ).observe(generation_duration)
                
                if STRUCTLOG_AVAILABLE:
                    self.logger.info(
                        "Report generated successfully",
                        format=format_type.value,
                        file_path=str(file_path),
                        generation_duration=generation_duration
                    )
                    
            except Exception as e:
                error_msg = f"Failed to generate {format_type.value} report: {str(e)}"
                generation_errors.append(error_msg)
                
                # Update Prometheus metrics
                if PROMETHEUS_AVAILABLE:
                    self.report_generation_counter.labels(
                        report_type=self.config.report_type.value,
                        format=format_type.value,
                        status="error"
                    ).inc()
                
                if STRUCTLOG_AVAILABLE:
                    self.logger.error(
                        "Report generation failed",
                        format=format_type.value,
                        error=str(e),
                        traceback=traceback.format_exc()
                    )
        
        # Send notifications if configured
        if self.config.slack_webhook_url or self.config.teams_webhook_url:
            self._send_report_notifications(report_data, executive_summary, generated_files)
        
        # Cleanup old reports if configured
        if self.config.artifact_retention_days > 0:
            self._cleanup_old_reports()
        
        if generation_errors and not generated_files:
            raise ValueError(f"All report generation failed: {'; '.join(generation_errors)}")
        
        if STRUCTLOG_AVAILABLE:
            self.logger.info(
                "Report generation completed",
                generated_formats=list(generated_files.keys()),
                total_files=len(generated_files),
                errors=len(generation_errors)
            )
        
        return generated_files
    
    def _generate_json_report(self, report_data: PerformanceReportData, executive_summary: Dict[str, Any]) -> Path:
        """Generate comprehensive JSON report with all performance data."""
        report_content = {
            "executive_summary": executive_summary,
            "report_metadata": {
                "report_id": report_data.report_id,
                "generation_timestamp": report_data.generation_timestamp.isoformat(),
                "report_version": report_data.report_version,
                "config": {
                    "report_name": self.config.report_name,
                    "report_type": self.config.report_type.value,
                    "variance_threshold": self.config.variance_threshold,
                    "memory_variance_threshold": self.config.memory_variance_threshold
                }
            },
            "performance_analysis": {
                "baseline_comparison": report_data.baseline_comparison,
                "variance_summary": report_data.variance_summary,
                "compliance_status": report_data.compliance_status,
                "quality_gates_status": report_data.quality_gates_status,
                "performance_grade": report_data.performance_grade.value
            },
            "trend_analysis": report_data.trend_analysis,
            "regression_detection": report_data.regression_detection,
            "ci_cd_integration": {
                "ci_cd_metrics": report_data.ci_cd_metrics,
                "build_information": report_data.build_information,
                "deployment_information": report_data.deployment_information,
                "quality_gate_results": report_data.quality_gate_results
            },
            "recommendations": {
                "critical_issues": report_data.critical_issues,
                "warning_issues": report_data.warning_issues,
                "performance_recommendations": report_data.performance_recommendations,
                "optimization_opportunities": report_data.optimization_opportunities
            },
            "statistical_metrics": {
                "statistical_confidence": report_data.statistical_confidence,
                "sample_size": report_data.sample_size,
                "test_duration_seconds": report_data.test_duration_seconds,
                "data_quality_score": report_data.data_quality_score
            }
        }
        
        # Generate filename with timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"performance_report_{timestamp}.json"
        file_path = self.config.output_directory / filename
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(report_content, f, indent=2, ensure_ascii=False, default=str)
        
        return file_path
    
    def _generate_html_report(self, report_data: PerformanceReportData, executive_summary: Dict[str, Any]) -> Path:
        """Generate comprehensive HTML report with charts and interactive elements."""
        if not JINJA2_AVAILABLE or not self.template_environment:
            # Fallback to basic HTML generation
            return self._generate_basic_html_report(report_data, executive_summary)
        
        try:
            template = self.template_environment.get_template("performance_report.html")
            
            html_content = template.render(
                report_data=report_data,
                executive_summary=executive_summary,
                config=self.config,
                generation_timestamp=datetime.now(timezone.utc)
            )
            
            # Generate filename with timestamp
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"performance_report_{timestamp}.html"
            file_path = self.config.output_directory / filename
            
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            return file_path
            
        except Exception as e:
            if STRUCTLOG_AVAILABLE:
                self.logger.warning("Template-based HTML generation failed, using fallback", error=str(e))
            return self._generate_basic_html_report(report_data, executive_summary)
    
    def _generate_basic_html_report(self, report_data: PerformanceReportData, executive_summary: Dict[str, Any]) -> Path:
        """Generate basic HTML report without templates."""
        html_content = f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{self.config.company_name} - Performance Analysis Report</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 0; padding: 20px; background: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
        .header {{ text-align: center; margin-bottom: 30px; padding-bottom: 20px; border-bottom: 2px solid #e0e0e0; }}
        .header h1 {{ color: #2c3e50; margin: 0; }}
        .summary {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin: 30px 0; }}
        .summary-card {{ background: #ecf0f1; padding: 20px; border-radius: 8px; text-align: center; }}
        .grade-A {{ color: #27ae60; }}
        .grade-B {{ color: #2ecc71; }}
        .grade-C {{ color: #f39c12; }}
        .grade-D {{ color: #e67e22; }}
        .grade-F {{ color: #e74c3c; }}
        .section {{ margin: 30px 0; }}
        .section h2 {{ color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px; }}
        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
        th {{ background: #34495e; color: white; }}
        .status-pass {{ color: #27ae60; font-weight: bold; }}
        .status-fail {{ color: #e74c3c; font-weight: bold; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>{self.config.company_name}</h1>
            <p>Performance Analysis Report - {report_data.generation_timestamp.strftime('%Y-%m-%d %H:%M:%S UTC')}</p>
        </div>
        
        <div class="section">
            <h2>Executive Summary</h2>
            <div class="summary">
                <div class="summary-card">
                    <h3>Overall Grade</h3>
                    <div class="grade-{executive_summary['performance_summary']['overall_grade']}">{executive_summary['performance_summary']['overall_grade']}</div>
                </div>
                <div class="summary-card">
                    <h3>Compliance Rate</h3>
                    <div>{executive_summary['performance_summary']['compliance_rate']:.1f}%</div>
                </div>
                <div class="summary-card">
                    <h3>Average Variance</h3>
                    <div>{executive_summary['performance_summary']['average_variance']:.2f}%</div>
                </div>
                <div class="summary-card">
                    <h3>Critical Issues</h3>
                    <div>{executive_summary['performance_summary']['total_critical_issues']}</div>
                </div>
            </div>
        </div>
        
        <div class="section">
            <h2>Performance Metrics</h2>
            <table>
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Status</th>
                        <th>Variance</th>
                    </tr>
                </thead>
                <tbody>"""
        
        # Add variance data to table
        for metric, compliant in report_data.compliance_status.items():
            variance_data = report_data.variance_summary.get(metric, {})
            variance = variance_data.get("variance_percent", 0) if isinstance(variance_data, dict) else 0
            status_class = "status-pass" if compliant else "status-fail"
            status_text = "PASS" if compliant else "FAIL"
            
            html_content += f"""
                    <tr>
                        <td>{metric.replace('_', ' ').title()}</td>
                        <td class="{status_class}">{status_text}</td>
                        <td>{variance:.2f}%</td>
                    </tr>"""
        
        html_content += """
                </tbody>
            </table>
        </div>
        
        <div class="section">
            <h2>Recommendations</h2>
            <ul>"""
        
        # Add recommendations
        for recommendation in report_data.performance_recommendations[:10]:  # Limit to top 10
            html_content += f"<li>{html.escape(recommendation)}</li>"
        
        html_content += f"""
            </ul>
        </div>
        
        <div style="text-align: center; margin-top: 40px; padding-top: 20px; border-top: 1px solid #e0e0e0; color: #7f8c8d;">
            <p>Report ID: {report_data.report_id}</p>
            <p>Generated by BF-refactor-merge Performance Testing Framework</p>
        </div>
    </div>
</body>
</html>"""
        
        # Generate filename with timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"performance_report_{timestamp}.html"
        file_path = self.config.output_directory / filename
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        return file_path
    
    def _generate_pdf_report(self, report_data: PerformanceReportData, executive_summary: Dict[str, Any]) -> Path:
        """Generate PDF report from HTML content."""
        if not WEASYPRINT_AVAILABLE:
            raise ValueError("PDF generation requires weasyprint library")
        
        # First generate HTML content
        html_file = self._generate_html_report(report_data, executive_summary)
        
        try:
            # Convert HTML to PDF
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            pdf_filename = f"performance_report_{timestamp}.pdf"
            pdf_path = self.config.output_directory / pdf_filename
            
            weasyprint.HTML(filename=str(html_file)).write_pdf(str(pdf_path))
            
            # Optionally remove HTML file if only PDF is needed
            if ReportFormat.HTML not in self.config.output_formats:
                html_file.unlink()
            
            return pdf_path
            
        except Exception as e:
            raise ValueError(f"PDF generation failed: {str(e)}")
    
    def _generate_markdown_report(self, report_data: PerformanceReportData, executive_summary: Dict[str, Any]) -> Path:
        """Generate Markdown report for documentation and version control."""
        md_content = f"""# {self.config.company_name} - Performance Analysis Report

**Generated:** {report_data.generation_timestamp.strftime('%Y-%m-%d %H:%M:%S UTC')}  
**Report ID:** {report_data.report_id}  
**Report Version:** {report_data.report_version}

## Executive Summary

| Metric | Value |
|--------|-------|
| Overall Grade | **{executive_summary['performance_summary']['overall_grade']}** |
| Compliance Rate | {executive_summary['performance_summary']['compliance_rate']:.1f}% |
| Average Variance | {executive_summary['performance_summary']['average_variance']:.2f}% |
| Critical Issues | {executive_summary['performance_summary']['total_critical_issues']} |
| Statistical Confidence | {executive_summary['performance_summary']['statistical_confidence']:.1f}% |
| Test Duration | {executive_summary['performance_summary']['test_duration_minutes']:.1f} minutes |

## Performance Metrics Analysis

| Metric | Status | Compliance | Variance |
|--------|--------|------------|----------|"""
        
        # Add variance data
        for metric, compliant in report_data.compliance_status.items():
            variance_data = report_data.variance_summary.get(metric, {})
            variance = variance_data.get("variance_percent", 0) if isinstance(variance_data, dict) else 0
            status = "âœ… PASS" if compliant else "âŒ FAIL"
            compliance = "Yes" if compliant else "No"
            
            md_content += f"\n| {metric.replace('_', ' ').title()} | {status} | {compliance} | {variance:.2f}% |"
        
        # Add key findings
        md_content += f"""

## Key Findings

- **Baseline Compliance:** {'âœ… Passed' if executive_summary['key_findings']['baseline_compliance'] else 'âŒ Failed'}
- **Regression Detected:** {'âš ï¸ Yes' if executive_summary['key_findings']['regression_detected'] else 'âœ… No'}
- **Performance Improvements:** {'ðŸŽ‰ Yes' if executive_summary['key_findings']['performance_improvements'] else 'No'}
- **Quality Gates:** {'âœ… Passed' if executive_summary['key_findings']['quality_gates_passed'] else 'âŒ Failed'}

"""
        
        # Add critical issues
        if report_data.critical_issues:
            md_content += "## Critical Issues\n\n"
            for i, issue in enumerate(report_data.critical_issues, 1):
                md_content += f"{i}. {issue}\n"
            md_content += "\n"
        
        # Add recommendations
        if report_data.performance_recommendations:
            md_content += "## Performance Recommendations\n\n"
            for i, recommendation in enumerate(report_data.performance_recommendations, 1):
                md_content += f"{i}. {recommendation}\n"
            md_content += "\n"
        
        # Add CI/CD integration info
        if report_data.ci_cd_metrics:
            md_content += "## CI/CD Integration\n\n"
            for key, value in report_data.ci_cd_metrics.items():
                md_content += f"- **{key.replace('_', ' ').title()}:** {value}\n"
            md_content += "\n"
        
        # Add trend analysis summary
        if report_data.trend_analysis:
            trend_summary = report_data.trend_analysis.get("trend_summary", {})
            if trend_summary:
                md_content += f"""## Trend Analysis

- **Overall Trend:** {trend_summary.get('overall_trend', 'unknown').title()}
- **Improving Metrics:** {trend_summary.get('improving_metrics', 0)}
- **Deteriorating Metrics:** {trend_summary.get('deteriorating_metrics', 0)}
- **Stable Metrics:** {trend_summary.get('stable_metrics', 0)}

"""
        
        md_content += f"""## Report Information

- **Data Quality Score:** {report_data.data_quality_score:.1f}/100
- **Sample Size:** {report_data.sample_size:,} requests
- **Statistical Confidence:** {report_data.statistical_confidence:.1f}%

---

*This report was generated automatically by the BF-refactor-merge Performance Testing Framework*
"""
        
        # Generate filename with timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"performance_report_{timestamp}.md"
        file_path = self.config.output_directory / filename
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(md_content)
        
        return file_path
    
    def _generate_csv_report(self, report_data: PerformanceReportData, executive_summary: Dict[str, Any]) -> Path:
        """Generate CSV report for data analysis and spreadsheet import."""
        # Generate filename with timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"performance_report_{timestamp}.csv"
        file_path = self.config.output_directory / filename
        
        with open(file_path, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            
            # Write header information
            writer.writerow(['Report Information'])
            writer.writerow(['Report ID', report_data.report_id])
            writer.writerow(['Generation Timestamp', report_data.generation_timestamp.isoformat()])
            writer.writerow(['Overall Grade', executive_summary['performance_summary']['overall_grade']])
            writer.writerow(['Compliance Rate (%)', f"{executive_summary['performance_summary']['compliance_rate']:.2f}"])
            writer.writerow(['Average Variance (%)', f"{executive_summary['performance_summary']['average_variance']:.2f}"])
            writer.writerow(['Critical Issues', executive_summary['performance_summary']['total_critical_issues']])
            writer.writerow(['Warning Issues', executive_summary['performance_summary']['total_warning_issues']])
            writer.writerow(['Statistical Confidence (%)', f"{executive_summary['performance_summary']['statistical_confidence']:.1f}"])
            writer.writerow([])  # Empty row
            
            # Write performance metrics
            writer.writerow(['Performance Metrics'])
            writer.writerow(['Metric', 'Status', 'Compliant', 'Variance (%)', 'Current Value', 'Baseline Value'])
            
            for metric, compliant in report_data.compliance_status.items():
                variance_data = report_data.variance_summary.get(metric, {})
                if isinstance(variance_data, dict):
                    variance = variance_data.get("variance_percent", 0)
                    current_value = variance_data.get("current_value", "N/A")
                    baseline_value = variance_data.get("baseline_value", "N/A")
                else:
                    variance = 0
                    current_value = "N/A"
                    baseline_value = "N/A"
                
                status = "PASS" if compliant else "FAIL"
                writer.writerow([
                    metric.replace('_', ' ').title(),
                    status,
                    compliant,
                    f"{variance:.2f}",
                    current_value,
                    baseline_value
                ])
            
            writer.writerow([])  # Empty row
            
            # Write quality gates
            if report_data.quality_gate_results:
                writer.writerow(['Quality Gates'])
                writer.writerow(['Gate Name', 'Status'])
                for gate, status in report_data.quality_gate_results.items():
                    writer.writerow([gate.replace('_', ' ').title(), status.upper()])
                writer.writerow([])  # Empty row
            
            # Write recommendations
            if report_data.performance_recommendations:
                writer.writerow(['Performance Recommendations'])
                writer.writerow(['Priority', 'Recommendation'])
                for i, recommendation in enumerate(report_data.performance_recommendations, 1):
                    writer.writerow([i, recommendation])
        
        return file_path
    
    def _send_report_notifications(self, report_data: PerformanceReportData, executive_summary: Dict[str, Any], generated_files: Dict[str, Path]) -> None:
        """Send report notifications to configured webhook endpoints."""
        notification_data = {
            "report_id": report_data.report_id,
            "timestamp": report_data.generation_timestamp.isoformat(),
            "overall_grade": executive_summary['performance_summary']['overall_grade'],
            "compliance_rate": executive_summary['performance_summary']['compliance_rate'],
            "critical_issues": executive_summary['performance_summary']['total_critical_issues'],
            "generated_files": [str(path) for path in generated_files.values()]
        }
        
        # Send Slack notification
        if self.config.slack_webhook_url:
            try:
                self._send_slack_notification(notification_data)
            except Exception as e:
                if STRUCTLOG_AVAILABLE:
                    self.logger.warning("Slack notification failed", error=str(e))
        
        # Send Teams notification
        if self.config.teams_webhook_url:
            try:
                self._send_teams_notification(notification_data)
            except Exception as e:
                if STRUCTLOG_AVAILABLE:
                    self.logger.warning("Teams notification failed", error=str(e))
    
    def _send_slack_notification(self, notification_data: Dict[str, Any]) -> None:
        """Send Slack webhook notification."""
        import requests
        
        color = "#36a64f" if notification_data["overall_grade"] in ["A", "B"] else "#ff9900" if notification_data["overall_grade"] == "C" else "#ff0000"
        
        slack_payload = {
            "attachments": [
                {
                    "color": color,
                    "title": "Performance Report Generated",
                    "fields": [
                        {"title": "Overall Grade", "value": notification_data["overall_grade"], "short": True},
                        {"title": "Compliance Rate", "value": f"{notification_data['compliance_rate']:.1f}%", "short": True},
                        {"title": "Critical Issues", "value": str(notification_data["critical_issues"]), "short": True},
                        {"title": "Report ID", "value": notification_data["report_id"], "short": True}
                    ],
                    "footer": "BF-refactor-merge Performance Testing",
                    "ts": int(datetime.fromisoformat(notification_data["timestamp"]).timestamp())
                }
            ]
        }
        
        response = requests.post(self.config.slack_webhook_url, json=slack_payload, timeout=10)
        response.raise_for_status()
    
    def _send_teams_notification(self, notification_data: Dict[str, Any]) -> None:
        """Send Microsoft Teams webhook notification."""
        import requests
        
        color = "00FF00" if notification_data["overall_grade"] in ["A", "B"] else "FFB366" if notification_data["overall_grade"] == "C" else "FF0000"
        
        teams_payload = {
            "@type": "MessageCard",
            "@context": "http://schema.org/extensions",
            "summary": "Performance Report Generated",
            "themeColor": color,
            "sections": [
                {
                    "activityTitle": "Performance Report Generated",
                    "activitySubtitle": f"Report ID: {notification_data['report_id']}",
                    "facts": [
                        {"name": "Overall Grade", "value": notification_data["overall_grade"]},
                        {"name": "Compliance Rate", "value": f"{notification_data['compliance_rate']:.1f}%"},
                        {"name": "Critical Issues", "value": str(notification_data["critical_issues"])},
                        {"name": "Timestamp", "value": notification_data["timestamp"]}
                    ]
                }
            ]
        }
        
        response = requests.post(self.config.teams_webhook_url, json=teams_payload, timeout=10)
        response.raise_for_status()
    
    def _cleanup_old_reports(self) -> None:
        """Cleanup old report files based on retention policy."""
        if self.config.artifact_retention_days <= 0:
            return
        
        cutoff_date = datetime.now() - timedelta(days=self.config.artifact_retention_days)
        deleted_count = 0
        
        for file_path in self.config.output_directory.glob("performance_report_*"):
            try:
                file_mtime = datetime.fromtimestamp(file_path.stat().st_mtime)
                if file_mtime < cutoff_date:
                    file_path.unlink()
                    deleted_count += 1
            except Exception as e:
                if STRUCTLOG_AVAILABLE:
                    self.logger.warning("Failed to delete old report file", file_path=str(file_path), error=str(e))
        
        if deleted_count > 0 and STRUCTLOG_AVAILABLE:
            self.logger.info("Cleaned up old report files", deleted_count=deleted_count, retention_days=self.config.artifact_retention_days)


def main():
    """
    Main entry point for the performance report generation script.
    
    Provides command-line interface for generating comprehensive performance reports
    with configurable output formats and integration options.
    """
    parser = argparse.ArgumentParser(
        description="Generate comprehensive performance analysis reports for BF-refactor-merge project",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Generate HTML and JSON reports
  python generate_reports.py --formats html json --output-dir ./reports
  
  # Generate executive summary with charts
  python generate_reports.py --report-type executive_summary --include-charts
  
  # Generate CI/CD integration report with notifications
  python generate_reports.py --report-type cicd_integration --slack-webhook https://hooks.slack.com/...
  
  # Generate baseline comparison with custom thresholds
  python generate_reports.py --variance-threshold 8.0 --memory-variance-threshold 12.0
        """
    )
    
    # Core configuration arguments
    parser.add_argument(
        "--report-name",
        default="Performance Analysis Report",
        help="Name of the generated report"
    )
    
    parser.add_argument(
        "--report-type",
        type=str,
        choices=[rt.value for rt in ReportType],
        default=ReportType.TECHNICAL_DETAILED.value,
        help="Type of report to generate"
    )
    
    parser.add_argument(
        "--formats",
        nargs="+",
        choices=[fmt.value for fmt in ReportFormat],
        default=["html", "json"],
        help="Output formats for the report"
    )
    
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("reports"),
        help="Output directory for generated reports"
    )
    
    # Data source arguments
    parser.add_argument(
        "--baseline-data",
        type=Path,
        help="Path to baseline performance data file"
    )
    
    parser.add_argument(
        "--test-results",
        type=Path,
        help="Path to performance test results file"
    )
    
    parser.add_argument(
        "--historical-data",
        type=Path,
        help="Path to historical performance data file"
    )
    
    # Report content arguments
    parser.add_argument(
        "--include-charts",
        action="store_true",
        default=True,
        help="Include performance charts in the report"
    )
    
    parser.add_argument(
        "--no-charts",
        action="store_true",
        help="Disable chart generation"
    )
    
    parser.add_argument(
        "--include-recommendations",
        action="store_true",
        default=True,
        help="Include performance recommendations"
    )
    
    parser.add_argument(
        "--include-trend-analysis",
        action="store_true",
        default=True,
        help="Include trend analysis"
    )
    
    # Performance threshold arguments
    parser.add_argument(
        "--variance-threshold",
        type=float,
        default=10.0,
        help="Performance variance threshold percentage (default: 10.0)"
    )
    
    parser.add_argument(
        "--memory-variance-threshold",
        type=float,
        default=15.0,
        help="Memory variance threshold percentage (default: 15.0)"
    )
    
    parser.add_argument(
        "--warning-variance-threshold",
        type=float,
        default=5.0,
        help="Warning variance threshold percentage (default: 5.0)"
    )
    
    # CI/CD and notification arguments
    parser.add_argument(
        "--ci-cd-integration",
        action="store_true",
        default=True,
        help="Include CI/CD integration data"
    )
    
    parser.add_argument(
        "--slack-webhook",
        type=str,
        help="Slack webhook URL for notifications"
    )
    
    parser.add_argument(
        "--teams-webhook",
        type=str,
        help="Microsoft Teams webhook URL for notifications"
    )
    
    parser.add_argument(
        "--retention-days",
        type=int,
        default=30,
        help="Report artifact retention in days (default: 30)"
    )
    
    # Template and styling arguments
    parser.add_argument(
        "--template-dir",
        type=Path,
        help="Custom template directory path"
    )
    
    parser.add_argument(
        "--company-name",
        default="BF-refactor-merge Project",
        help="Company name for report branding"
    )
    
    # Logging and debug arguments
    parser.add_argument(
        "--verbose",
        "-v",
        action="store_true",
        help="Enable verbose logging"
    )
    
    parser.add_argument(
        "--log-file",
        type=Path,
        help="Log file path for detailed logging"
    )
    
    args = parser.parse_args()
    
    # Setup logging
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler(args.log_file) if args.log_file else logging.NullHandler()
        ]
    )
    
    try:
        # Create report configuration
        config = ReportConfiguration(
            report_name=args.report_name,
            report_type=ReportType(args.report_type),
            output_formats=[ReportFormat(fmt) for fmt in args.formats],
            output_directory=args.output_dir,
            baseline_data_path=args.baseline_data,
            test_results_path=args.test_results,
            historical_data_path=args.historical_data,
            include_charts=args.include_charts and not args.no_charts,
            include_recommendations=args.include_recommendations,
            include_trend_analysis=args.include_trend_analysis,
            variance_threshold=args.variance_threshold,
            memory_variance_threshold=args.memory_variance_threshold,
            warning_variance_threshold=args.warning_variance_threshold,
            ci_cd_integration=args.ci_cd_integration,
            slack_webhook_url=args.slack_webhook,
            teams_webhook_url=args.teams_webhook,
            artifact_retention_days=args.retention_days,
            template_directory=args.template_dir,
            company_name=args.company_name
        )
        
        # Initialize report generator
        generator = PerformanceReportGenerator(config)
        
        # Collect performance data
        print("Collecting performance data...")
        report_data = generator.collect_performance_data()
        
        # Generate reports
        print(f"Generating reports in formats: {args.formats}")
        generated_files = generator.generate_reports(report_data)
        
        # Display results
        print("\nReport generation completed successfully!")
        print(f"Overall Performance Grade: {report_data.calculate_overall_performance_grade().value}")
        print(f"Statistical Confidence: {report_data.statistical_confidence:.1f}%")
        print(f"Data Quality Score: {report_data.data_quality_score:.1f}/100")
        
        print("\nGenerated files:")
        for format_name, file_path in generated_files.items():
            print(f"  {format_name.upper()}: {file_path}")
        
        if report_data.critical_issues:
            print(f"\nâš ï¸  Critical Issues Found ({len(report_data.critical_issues)}):")
            for issue in report_data.critical_issues[:5]:  # Show first 5
                print(f"  - {issue}")
        
        if report_data.performance_recommendations:
            print(f"\nðŸ’¡ Top Recommendations:")
            for recommendation in report_data.performance_recommendations[:3]:  # Show top 3
                print(f"  - {recommendation}")
        
        return 0
        
    except Exception as e:
        print(f"\nError: Report generation failed: {str(e)}", file=sys.stderr)
        if args.verbose:
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())